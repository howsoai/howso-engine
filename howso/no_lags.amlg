(null

	;small method to prototype forecasting with no lags.
	#react_series_no_lags
	(declare
		(assoc
			;{type "list" values "string"}
			series_context_features (list)
			;{type "list" values {type "list" values "any"}}
			series_context_values (list)
			;{type "list" values "string"}
			action_features (list)
			;{type "boolean"}
			continue_series .true
			;{type "number"}
			desired_conviction (null)
		)
		; (call !ValidateParameters)

		(let
			(assoc
				original_series_context_features series_context_features
				time_feature !tsTimeFeature
			)

			;use all the features including all the derived ones (but not lags, these wouldn't exist in theory)
			(assign (assoc
				series_context_features
					(filter
						(lambda (and
							(!= "lag" (get !featureAttributes [(current_value 1) "ts_type"]))
							(not (contains_value [".reverse_series_index" ".synchronous_counter"] (current_value)))
						))
						!trainedFeatures
					)
			))

			(assign (assoc
				series_context_values
					(call !DeriveUntrainedSeriesData (assoc
						series_data series_context_values
						series_data_features original_series_context_features
						features_to_retrieve series_context_features
						input_is_substituted input_is_substituted
					))
			))
		)

		(declare (assoc
			recents_bandwidth 3
			iwe_threshold 0.4
			series_id_features (get !tsFeaturesMap "series_id_features")
		))

		(declare (assoc
			recent_timesteps (tail series_context_values recents_bandwidth)
			other_timesteps (trunc series_context_values (* -1 recents_bandwidth))
		))

		(declare  (assoc
			hyperparam_map
				(call !GetHyperparameters (assoc
					context_features features
					weight_feature weight_feature
				))
			series_id_features (get !tsFeaturesMap "series_id_features")
		))

		;Must find the collection of indices from non-recent cases to keep
		(declare (assoc
			other_timestep_idxs_to_keep
				(filter
					(lambda
						(call !ShouldTimestepBeUsed (assoc
							context_features series_context_features
							context_values (get other_timesteps (current_value 1))
							hyperparam_map hyperparam_map
						))
					)
					(indices other_timesteps)
				)
		))

		(declare (assoc
			context_timesteps
				(append
					(unzip other_timesteps other_timestep_idxs_to_keep)
					recent_timesteps
				)
		))

		(declare (assoc
			timesteps_influential_cases
				(map
					(lambda
						(get
							(call !SingleReact (assoc
								context_features series_context_features
								context_values (current_value 1)
								desired_conviction desired_conviction
								details
									{
										"influential_cases" .true
										"influential_cases_raw_weights" .true
									}
							))
							"influential_cases"
						)
					)
					context_timesteps
				)
		))

		(declare (assoc
			;map of influential series to its raw influence weight (un-normalized)
			;keys / influential series are represented as [ [series_id_values] time_value]
			;values are the numeric accumulated un-normalized raw influence weights
			influential_series_map {}
			time_feature !tsTimeFeature
		))

		;now with a list of lists of influential cases for each context timestep,
		;a set of "influential_series" must be created, by combining influential cases of the same series
		;together and accumulating their influences.
		(map
			(lambda
				(let
					(assoc
						influential_cases_for_timestep (current_value 1)
						timestep (get context_timesteps (current_index 1))
					)

					(map
						(lambda
							(let
								(assoc
									influential_case (current_value 1)
								)

								(declare (assoc
									time_value (get influential_case time_feature)
									series_id_values (unzip influential_case series_id_features)
									;TODO: this should be raw... but they are all zero right now so idk
									influence_value (get influential_case ".influence_weight")
								))

								(declare (assoc
									matching_influential_series_map
										(filter
											;to accumulate to an existing influential_series, this timestep must both be
											;the same series and be later in time.
											(lambda (and
												(= series_id_values (first (current_index)))
												(> time_value (last (current_index)))
											))
											influential_series_map
										)
								))

								(if (> (size matching_influential_series_map) 1)
									(print "this is not nice.")
								)

								;register a new influential_series
								(accum (assoc
									influential_series_map
										(if (size matching_influential_series_map)
											;accum to the matching inf series (hopefully there is only one)
											(associate
												(first (indices matching_influential_series_map))
												(+
													(get matching_influential_series_map [(first (indices matching_influential_series_map))])
													influence_value
												)
											)

											;register a new influential_series if no existing registries match
											(associate
												[series_id_values time_value] influence_value
											)
										)
								))
							)
						)
						influential_cases_for_timestep
					)
				)
			)
			timesteps_influential_cases
		)


		;TODO: We can "re-weight" the influences for each series by computing a DTW distance between timesteps of the
		;influential series and the given/context timesteps.
		;For now we will just use these influences.

		;TODO: Now we must "interpolate" the next timestep using the collection of influential series.
		;To do so, we need one timestep per influential series whose values will be in the interpolation
		;For each each series, I will get the last series index that was influential, and grab the *next* case (if it exists)
		(declare (assoc
			influential_series_next_timestep_map
				(filter (map
					(lambda
						(let
							(assoc
								series_id_values (first (current_index 1))
							)

							(declare (assoc
								max_series_index_observed
									(apply "max"
										(map
											(lambda
												(get (current_value) ".series_index")
											)
											(filter
												(lambda (= series_id_values (unzip (current_value) series_id_features)))
												(apply "append" timesteps_influential_cases)
											)
										)
									)
							))

							(declare (assoc
								next_timestep_case_id
									(first (contained_entities
										(values (map
											(lambda (query_equals (current_index) (current_value)))
											(zip series_id_features series_id_values)
										))
										(query_equals ".series_index" (+ max_series_index_observed 1))
									))
							))

							(if next_timestep_case_id
								(append
									(zip series_context_features (retrieve_from_entity next_timestep_case_id series_context_features))
									{".case_id" next_timestep_case_id}
								)
							)
						)
					)
					influential_series_map
				))
		))

		(declare (assoc
			norm_influential_series_map
				(normalize (map
					(lambda (get influential_series_map [(current_index 1)]))
					influential_series_next_timestep_map
				))
			influential_series (indices influential_series_next_timestep_map)
		))

		{
			"action_values"
				(map
					(lambda
						(call !InterpolateActionValues (assoc
							action_feature (current_value 1)
							candidate_case_ids
								(map
									(lambda (get influential_series_next_timestep_map [(current_value 2) ".case_id"]))
									influential_series
								)
							candidate_case_weights (map (lambda (get norm_influential_series_map [(current_value 1)])) influential_series)
							candidate_case_values
								(map
									(lambda (get influential_series_next_timestep_map [(current_value 2) (current_value 2)]))
									influential_series
								)
						))
					)
					action_features
				)
		}
	)


	;boolean returning helper func to help determine whether timesteps should
	;be kept for use as contexts in the no-lag flow.
	#!ShouldTimestepBeUsed
	(declare
		(assoc
			context_features []
			context_values []

			hyperparam_map {}
		)

		;should probably do something marginally relatable to the reduction function.
		;since that is still significantly in motion, instead will just use some randomness
		(< (rand) 0.3)
	)
)