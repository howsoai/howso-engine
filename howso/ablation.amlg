;Contains methods for ablation flows.
(null

	;compute the entropy of the influence weights of the context's influential cases. equidistant neighbors = high entropy
	; features: list of features to react to
	; feature_values: optional list of feature values to react to
	; case_id: optional case id in the dataset to react to
	; use_case_weights: optional flag default true. if false, case weights will not be used during the react
	; weight_feature: optional, default '.case_weight'. name of feature whose values to use as case weights
	; output_most_influential_only: optional, boolean, default false. outputs influential cases' entropy and neighbor info
	;	when true, outputs a tuple of [ entropy, most influential case id, influence of most influential ]
	;	when false, outputs a tuple of [ entropy, most influential case id, influence of most influential, influence of least influential, is_duplicate]
	#!ComputeInfluenceWeightEntropyTuple
	(declare
		(assoc
			features (list)
			feature_values (null)
			case_id (null)
			use_case_weights .true
			weight_feature !autoAblationWeightFeature
			output_most_influential_only .false
		)

		(if (not use_case_weights)
			(assign (assoc weight_feature ".none"))
		)

		(declare (assoc
			local_cases_pairs
				(compute_on_contained_entities
					(if case_id (query_not_in_entity_list [case_id]) [])
					(query_nearest_generalized_distance
						closest_k
						features
						(if case_id case_id feature_values)
						p_parameter
						feature_weights
						!queryDistanceTypeMap
						query_feature_attributes_map
						feature_deviations
						(null)
						dt_parameter
						(if use_case_weights weight_feature)
						(rand)
						(null) ;radius
						!numericalPrecision
						;output as ordered pair
						.true
					)
				)
		))

		;if feature_values are not null, we are reacting to a new case. We ablate duplicate identical cases.
		;and a case is identical if its probability is >=1 or influence is .infinity
		(if (!= feature_values (null))
			(if (= .infinity (first (last local_cases_pairs)) )
				(conclude [ .infinity (null) ])

				(and
					(>= (first (last local_cases_pairs)) 1)
					(= "surprisal_to_prob" dt_parameter)
				)
				(conclude [ .infinity (null) ])
			)
		)

		;outputs the tuple of [ entropy, most influential case id, influence of most influential ]
		(if output_most_influential_only
			(conclude [ (entropy (normalize (last local_cases_pairs)))  (first (first local_cases_pairs))  (first (last local_cases_pairs)) ] )
		)

		(declare (assoc closest_influence (first (last local_cases_pairs)) ))

		;true if this case has exact duplicates
		(declare (assoc
			is_duplicate
				(or
					(= .infinity closest_influence)
					(and
						(= "surprisal_to_prob" dt_parameter)
						(>= closest_influence 1)
					)
				)
		))

		;output tuple of: [ entropy, most influential case id, influence of most influential, influence of least influential, is_duplicate]
		[
			(entropy (normalize (last local_cases_pairs)))
			(first (first local_cases_pairs))
			closest_influence
			(last (last local_cases_pairs))
			is_duplicate
		]
	)

	;used to compute and store the influence weight entropies (IWE) for all cases contained in the Trainee in #Analyze
	; features: list of features to use when determining influential cases.
	; label_name: optional name of the feature to store influence weight entropies in.
	; compute_all: optional, boolean. if set to true will compute IWE for all cases.
	;	default is false, which samples up to !autoAblationInfluenceWeightEntropySampleSize (default of 2000) cases
	; specific_case_ids: optional, list of specific case_ids for which to compute. only applicable if compute_all is also true
	#!ComputeAndStoreInfluenceWeightEntropies
	(declare
		(assoc
			label_name (null)
			compute_all .false
			specific_case_ids (null)
		)
		(declare (assoc
			influence_weight_entropy_map
				||(map
					(lambda
						(call !ComputeInfluenceWeightEntropyTuple (assoc
							case_id (current_index 1)
							features features
							use_case_weights use_case_weights
							weight_feature weight_feature
						))
					)
					(zip
						;datasets that are < !autoAblationInfluenceWeightEntropySampleSize + 2% in size can compute IWE for all cases
						(if (or
								compute_all
								(<
									(call !GetNumTrainingCases)
									(* 1.02 !autoAblationInfluenceWeightEntropySampleSize)
								)
							)
							(or specific_case_ids (call !AllCases))

							;else compute on a random sample of !autoAblationInfluenceWeightEntropySampleSize cases
							(call !AllCases (assoc
								num !autoAblationInfluenceWeightEntropySampleSize
								rand_seed (rand)
							))
						)
					)
				)
		))

		(call !StoreCaseValues (assoc
			case_values_map
				(if (or
						compute_all
						(<
							(call !GetNumTrainingCases)
							(* 1.02 !autoAblationInfluenceWeightEntropySampleSize)
						)
					)
					(map (lambda (first (current_value))) influence_weight_entropy_map)

					;explicitly store null in all the other cases by appending the computed entropy map to a zipped assoc of all case ids
					(append
						(zip (call !AllCases))
						(map (lambda (first (current_value))) influence_weight_entropy_map)
					)
				)
			label_name (or label_name !internalLabelInfluenceWeightEntropy)
		))

		(declare (assoc
			perfect_match_influence (if (= "surprisal_to_prob" dt_parameter) 1 .infinity)
		))
		;compute the 90%th quantile for closest case influence, so that only cases that are in the 10% closest are considered
		(declare (assoc
			ninety_percent_quantile_closest_influence
				(quantile
					(values (filter
						(lambda (!= perfect_match_influence (current_value)))
						(map (lambda (get (current_value) 2)) influence_weight_entropy_map)
					))
					0.9
				)
		))

		;recompute this value for the entire dataset
		(assign_to_entities (assoc
			!autoAblationMaxInfluenceWeightEntropy (call !RecomputeAndCacheMaxInfluenceWeightEntropy)
		))

		;do not compute the 90% quantile for just the specific cases
		(if (= (null) specific_case_ids)
			(assign_to_entities (assoc
				!autoAblationNinetyPercentQuantileInfluence ninety_percent_quantile_closest_influence
			))
		)

		;if computing on the entire dataset, mark cases that are very close to exactly one other case and can be merged together
		(if (and compute_all (= (null) specific_case_ids))
			(declare (assoc
				near_duplicate_map
					(let
						(assoc
							;from the 10% closest (ignoring duplicate cases), only keep those whose influence weight entropy is under 0.1,
							;denoting that they are very close to just their one nearest neighbor
							extremely_close_to_one_neighbor_map
								(filter
									(lambda (and
										;closest case influence is in the top 10%
										(> (get (current_value) 2) ninety_percent_quantile_closest_influence)
										;entropy is < 0.1, meaning significantly closer to one case than any of the others
										(< (first (current_value)) 0.1)
										;ignore duplicates since they'll be merged together separately
										(!= perfect_match_influence (get (current_value) 2))
									))
									influence_weight_entropy_map
								)
						)

						;create an assoc of case -> neighbor_case to merge into
						;filters out null if case is reciprocated, but this case has higher entropy and will absorb the one with the lower entropy
						(filter (map
							(lambda (let
								(assoc nearest_case (get (current_value 1) 1))
								;this case is reciprocated as the closest by nearest_case
								(if (= (current_index) (get extremely_close_to_one_neighbor_map [nearest_case 1]))
									;if the entropy of this case is smaller, it should be merged into nearest_case, else it should not be merged
									(if (<= (first (current_value)) (get extremely_close_to_one_neighbor_map [nearest_case 0]))
										nearest_case
									)

									nearest_case
								)
							))
							extremely_close_to_one_neighbor_map
						))
					)
			))
		)

		(if compute_all
			;output an assoc of case id -> "duplicate" or "too_far_for_removal" or "near_duplicate" (for extremely close single neighbors), depending if any cases match that criteria
			; "too_far_for_removal" applies to any case that should not be removed because its closest case is relatively too far (farther that its farthest)
			; "near_duplicate" applies to any cases that are extremely close to exactly one neighbor case
			(filter (map
				(lambda
					;(current_value) is a tuple of [ entropy, closest_case_id, closest_influence, farthest_influence, is_duplicate ]

					;output "duplicate" if case is duplate otherwise output "too_far_for_removal" if case is too far to be removed
					(if (last (current_value))
						"duplicate"

						;else if this case is too far for removal, if its probability (inverse distance) is too small
						(<
							;closest case probability (inverse distance)
							(get (current_value) 2)
							;closest case's farthest neighbor influence probability (inverse distance)
							(get influence_weight_entropy_map [ (get (current_value 1) 1) 3 ])
						)
						"too_far_for_removal"

						(contains_index near_duplicate_map (current_index))
						"near_duplicate"
					)
				)
				influence_weight_entropy_map
			))
		)
	)

	;initialize parameters and internal features related to auto ablation
	; weight_feature: optional, default '.case_weight'. name of feature whose values to use as case weights
	#!InitializeAutoAblation
	(declare
		(assoc weight_feature !autoAblationWeightFeature)

		(assign_to_entities (assoc !hasPopulatedCaseWeight .true ))
		(call !CreateCaseWeights (assoc feature_name weight_feature) )
	)

	;check the relative threshold map against the the current prediction stats map.
	;
	;parameters:
	; abs_threshold_map: a map of measure to a map of feature name to absolute score
	; prediction_stats_map: the prediction stats map.
	#!CheckAbsThresholds
	(declare
		(assoc
			abs_threshold_map (assoc)
			prediction_stats_map (assoc)
		)
		(map
			(lambda (map
				(lambda
					;If we are computing for rmse or r2 we check if we're over the threshold.
					; Otherwise, check that we're under.

					;(current_index 1) is the top-level index of abs_threshold_map, which is
					; comprised of the different measures (accuracy, precision, recall, rsmse... etc.)
					; that have thresholds defined.
					(if (contains_value (list "rmse" "r2") (current_index 1))
						;Use (current_index 2) and (current_index 1) to access the values in the prediction stats
						; maps, which have the same structure as the threshold map.

						;(current_value) is the numerical value of the threshold. Exceeding this should satisfy the
						; threshold and stop reduction.
						(>
							(get prediction_stats_map (list (current_index 2) (current_index 1)))
							(current_value)
						)
						(<
							(get prediction_stats_map (list (current_index 2) (current_index 1)))
							(current_value)
						)
					)
				)
				;(current_value) contains the threshold map for a specific measure, e.g. for accuracy {target 0.75}
				; would indicate an accuracy value of less than 0.75 would satisfy the threshold and stop reduction.
				(current_value)
			))
			abs_threshold_map
		)
	)

	;check the relative threshold map against the the current and previous prediction stats maps.
	;
	;parameters:
	; delta_threshold_map: a map of measure to a map of feature name to relative score
	; prev_prediction_stats_map: the previous prediction stats map.
	; new_prediction_stats_map: the new prediction stats map.
	#!CheckDeltaThresholds
	(declare
		(assoc
			delta_threshold_map (assoc)
			prev_prediction_stats_map (assoc)
			new_prediction_stats_map (assoc)
		)
		(map
			(lambda (map
				(lambda
					(>
						;If we are computing the delta on rmse or r2, subtract the prev from the new
						; and vice versa if we are not to ensure a positive value to trigger any thresholds.

						;(current_index 1) is the top-level index of delta_threshold_map, which is
						; comprised of the different measures (accuracy, precision, recall, rsmse... etc.)
						; that have thresholds defined.
						(if (contains_value (list "rmse" "r2") (current_index 1))
							;Use (current_index 2) and (current_index 1) to access the values in the prediction stats
							; maps, which have the same structure as the threshold map.
							(-
								(get new_prediction_stats_map (list (current_index 2) (current_index 1)))
								(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
							)
							(-
								(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
								(get new_prediction_stats_map (list (current_index 2) (current_index 1)))
							)
						)
						;(current_value) is the numerical value of the threshold. Exceeding this should satisfy the
						; threshold and stop reduction.
						(current_value)
					)
				)
				;(current_value) contains the threshold map for a specific measure, e.g. for accuracy {target 0.05}
				; would indicate a delta of greater than 0.05 would satisfy the threshold and stop reduction.
				(current_value)
			))
			delta_threshold_map
		)
	)

	;check the relative threshold map against the the current and previous prediction stats maps.
	;
	;parameters:
	; rel_threshold_map: a map of measure to a map of feature name to relative score
	; prev_prediction_stats_map: the previous prediction stats map.
	; new_prediction_stats_map: the new prediction stats map.
	#!CheckRelThresholds
	(declare
		(assoc
			rel_threshold_map (assoc)
			prev_prediction_stats_map (assoc)
			new_prediction_stats_map (assoc)
		)
		(map
			(lambda
				(map
					(lambda
						(>
							;If we are computing relative change in rmse or r2, then compute the
							; change as-is. If not, then we multiply the resulting % change by -1
							; since a negative  % change needs to trigger the threshold.

							;(current_index 1) is the top-level index of rel_threshold_map, which is
							; comprised of the different measures (accuracy, precision, recall, rsmse... etc.)
							; that have thresholds defined.
							(if (contains_value (list "rmse" "r2") (current_index 1))
								;Use (current_index 2) and (current_index 1) to access the values in the prediction stats
								; maps, which have the same structure as the threshold map.
								(/
									(-
										(get new_prediction_stats_map (list (current_index 2) (current_index 1)))
										(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
									)
									(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
								)
								(- (/
									(-
										(get new_prediction_stats_map (list (current_index 2) (current_index 1)))
										(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
									)
									(get prev_prediction_stats_map (list (current_index 2) (current_index 1)))
								))
							)
							;(current_value) is the numerical value of the threshold. Exceeding this should satisfy the
							; threshold and stop reduction.
							(current_value)
						)
					)
					;(current_value) contains the threshold map for a specific measure, e.g. for accuracy {target 0.05}
					; would indicate a % change of greater than 5% would satisfy the threshold and stop reduction.
					(current_value)
				)
			)
			rel_threshold_map
		)
	)

	;helper method which flattens the result of one of the !Check*Thresholds methods
	; to determine whether at least one of the thresholds was violated .true or not .false.
	#!FlattenThresholdResultMap
	(declare
		(assoc
			threshold_result_map (assoc)
		)
		(apply "or"
			(values (map
				(lambda
					(apply "or" (values (current_value)))
				)
				threshold_result_map
			))
		)
	)

	;check all of the threshold maps against the current prediction stats map or the current
	; and previous prediction stats maps. Returns the threshold map that failed.
	;
	;parameters:
	; abs_threshold_map: a map of a measure to a map of feature name to absolute score threshold
	; delta_threshold_map: a map of measure to a map of feature name to delta score threshold
	; rel_threshold_map: a map of measure to a map of feature name to relative score threshold
	; prev_prediction_stats_map: the previous prediction stats map.
	; new_prediction_stats_map: the new prediction stats map.
	#!CheckThresholds
	(declare
		(assoc
			abs_threshold_map (assoc)
			delta_threshold_map (assoc)
			rel_threshold_map (assoc)
			prev_prediction_stats_map (assoc)
			new_prediction_stats_map (assoc)
		)

		(declare (assoc
			abs_threshold_result_map
				(call !CheckAbsThresholds (assoc
					abs_threshold_map abs_threshold_map
					prediction_stats_map new_prediction_stats_map
				))
			delta_threshold_result_map
				(call !CheckDeltaThresholds (assoc
					delta_threshold_map delta_threshold_map
					prev_prediction_stats_map prev_prediction_stats_map
					new_prediction_stats_map new_prediction_stats_map
				))
			rel_threshold_result_map
				(call !CheckRelThresholds (assoc
					rel_threshold_map rel_threshold_map
					prev_prediction_stats_map prev_prediction_stats_map
					new_prediction_stats_map new_prediction_stats_map
				))
		))

		(append
			(if (call !FlattenThresholdResultMap (assoc threshold_result_map abs_threshold_result_map))
				{"abs" abs_threshold_result_map}
				{}
			)
			(if (call !FlattenThresholdResultMap (assoc threshold_result_map delta_threshold_result_map))
				{"delta" delta_threshold_result_map}
				{}
			)
			(if (call !FlattenThresholdResultMap (assoc threshold_result_map rel_threshold_result_map))
				{"rel" rel_threshold_result_map}
				{}
			)
		)
	)

	;reduce the trained data by removing cases that match the following criteria:
	;a) it's an exact duplicate
	;b) a near-duplicate (very similar to exactly one other case)
	;c) it's not "too far" away (keep outliers and dissimilar cases)
	;d) its influence weight entropy is relatively high (above a threshold, case can be evenly distributed among its neighbors)
	;{long_running .true}
	#reduce_data
	(declare
		;returns {
		; 	type "assoc"
		;	additional_indices .false
		; 	indices {
		; 		"threshold_info" {
		; 			type "assoc"
		; 			description
		; 				(concat
		; 					"A map of threshold-type (abs, relative, or delta) to map of metric to map of feature name to boolean. Indicating "
		; 					"what thresholds were met to trigger the end of data reduction."
		; 				)
		; 			additional_indices {
		; 				type "assoc"
		; 				description "A threshold of metric (accuracy, r2, etc) to map of feature name to boolean."
		; 				additional_indices {
		; 					type "assoc"
		; 					additional_indices "boolean"
		; 					description "A map of feature names to booleans indicating if the threshold for this feature was met."
		; 				}
		; 			}
		; 		}
		; 	}
		; }
		(assoc
			;{type "string"}
			;name of feature whose values to use as case weights, defaults to ".case_weight"
			distribute_weight_feature !autoAblationWeightFeature
			;{type "list" values "string"}
			;list of features to use when computing influence weight entropies, defaults to all trained features
			features !trainedFeatures
			;{type "number"}
			;stores the maximum number of cases that may remain after data is reduced
			;	default to the value stored within the Trainee via 'set_auto_ablation_params', which defaults to 50,000.
			reduce_max_cases !postReduceMaxCases
			;{type "boolean"}
			;skip auto analyzing as cases are removed
			skip_auto_analyze .false
		)
		(call !ValidateParameters)

		;reset !dataMassChangeSinceLastDataReduction since we are now reducing data
		(assign_to_entities (assoc !dataMassChangeSinceLastDataReduction 0.0 ))

		;Declare variables for internal use.
		(declare (assoc
			output (assoc)
		))

		;ensure it's an integer
		(assign (assoc reduce_max_cases (floor reduce_max_cases) ))

		;Ensure that ablation is initialized before we begin.
		(if (not !hasPopulatedCaseWeight)
			(call !InitializeAutoAblation (assoc
				weight_feature distribute_weight_feature
			))
		)
		(declare (assoc num_cases (call !GetNumTrainingCases) ))

		;nothing needed to reduce since the dataset is already small enough
		(if (>= (max reduce_max_cases !autoAblationMinNumCases) num_cases)
			(conclude
				(call !Return (assoc payload output))
			)
		)

		(declare (assoc
			hyperparam_map (call !GetHyperparameters (assoc weight_feature distribute_weight_feature))
		))

		(declare (assoc
			k_parameter (get hyperparam_map "k")
			p_parameter (get hyperparam_map "p")
			feature_weights (get hyperparam_map "featureWeights")
			dt_parameter (get hyperparam_map "dt")
			feature_deviations (get hyperparam_map "featureDeviations")
			query_feature_attributes_map (get hyperparam_map "featureDomainAttributes")

			all_case_ids (call !AllCases)
			done .false
		))

		;cache surprisals to the nearest 100 for each case
		;any cases that aren't in their most similar 100 can be considered to be 'too far away'
		(declare (assoc
			neighbor_surprisals_map
				||(map
					(lambda
						(compute_on_contained_entities
							(query_not_in_entity_list [(current_index 1)])
							(query_nearest_generalized_distance
								100
								features
								(current_index)
								p_parameter
								feature_weights
								!queryDistanceTypeMap
								query_feature_attributes_map
								feature_deviations
								(null)
								(if (= "surprisal_to_prob" dt_parameter) "surprisal" 1)
								distribute_weight_feature
								(rand)
								(null) ;radius
								!numericalPrecision
							)
						)
					)
					(zip all_case_ids)
				)
		))

		;mark each case as not being kept at first
		(if (= (size (contained_entities (query_exists ".keeping"))) 0)
			;create the label because it doesn't exist yet
			(map
				(lambda
					(accum_entity_roots (current_value) (zip_labels
						[".keeping"] [(null)]
					))
				)
				all_case_ids
			)
		)

		(declare (assoc
			min_neighbor_surprisals
				(map
					(lambda (apply "min" (values (current_value))))
					neighbor_surprisals_map
				)
		))

		;store the surprisal to each cases's most similar neighbor
		(call !StoreCaseValues (assoc
			case_values_map min_neighbor_surprisals
			label_name ".neighbor_surprisal"
		))

		(declare (assoc
			;map of case to its core-set surprisal (the min surprisal to any case in the coreset for all cases)
			;set it to an extremely large value
			case_to_css_map (map 10e13 (zip all_case_ids))

			;Experimental parameter: the amount of lowest neighbor-surprisal cases to consider for keeping
			;CAN BE NULLED
			lowest_ns_cases_trunc_n (null)

			;maximum number of cases to select for keeping per iteration
			max_cases_to_keep_per_iter
				(min
					30
					(ceil (/ reduce_max_cases 100))
				)

			;numeric value for the coreset-surprisal to neighbor-surprisal ratio
			;if the maximum ratio among non core-set features is lower than this value and
			;enough cases have already been selected, then selection is ended.
			ratio_cutoff_value 5.0
		))

		(while (not done)
			(let
				(assoc
					cases_to_add
						(if (= (current_index 1) 0)
							;on first iteration, just take lowest DC case
							(contained_entities
								(query_equals ".keeping" (null))
								(query_min ".neighbor_surprisal" 1 .true)
							)

							;otherwise need cases with low neighbor surprisal (ns) that is far from its most similar case in current_cases_to_keep
							(let
								(assoc
									low_ns_case_scores_map
										(map
											(lambda
												;coreset surprisal / neighbor surprisal, the smaller the neighbor surprisal the larger this score
												(/
													(get case_to_css_map (current_index))
													(get min_neighbor_surprisals (current_index))
												)
											)
											(compute_on_contained_entities
												(query_equals ".keeping" (null) )
												(if lowest_ns_cases_trunc_n
													(query_min ".neighbor_surprisal" lowest_ns_cases_trunc_n .true)
												)
											)
										)
								)

								(declare (assoc
									coreset_size
										(if lowest_ns_cases_trunc_n
											(size (contained_entities (query_not_equals ".keeping" (null) )))
											(- num_cases (size low_ns_case_scores_map))
										)
								))

								(declare (assoc
									num_cases_to_keep
										(min
											;grow coreset in relation to its current size.
											(max
												1
												(floor (/ coreset_size 10))
											)
											;or use the maximum number of cases per iteration
											max_cases_to_keep_per_iter
											;if approaching max size, select as many cases needed to reach limit.
											(- reduce_max_cases coreset_size)
										)
								))

								;if enough cases have been selected, and ratios have fallen below the cutoff, then end case selection
								;after this iteration.
								(if (and
										(>= coreset_size !autoAblationMinNumCases)
										(<
											(apply "max" (values low_ns_case_scores_map))
											ratio_cutoff_value
										)
									)
									(assign (assoc done .true))
								)

								;sorting low dc cases by *decreasing* "score" and return the right amount
								(if (= 1 num_cases_to_keep)
									(trunc (index_max low_ns_case_scores_map) 1)

									(sort
										(lambda
											(-
												(get low_ns_case_scores_map (current_value 1))
												(get low_ns_case_scores_map (current_value))
											)
										)
										(indices low_ns_case_scores_map)
										num_cases_to_keep
									)
								)
							)
						)
					iteration (current_index 1)
				)

				;mark new cases to keep by setting the iteration value when these cases were selected into the core set
				(map
					(lambda
						(assign_to_entities (current_value) (assoc ".keeping" iteration))
					)
					cases_to_add
				)

				(assign (assoc
					case_to_css_map
						;for every non-coreset case, store min of newly computed min css and old min css
						||(map
							(lambda
								(min
									;for each non-coreset case, determine surprisal to its closest case in the coreset
									(or
										;get the case's min surprisal to any of the cases_to_add, and if they are all too far away,
										;just output an exetremely large value
										(apply "min" (values
											(unzip
												(get neighbor_surprisals_map (current_index))
												cases_to_add
											)
										))
										;extremeley large value
										10e13
									)
									(get case_to_css_map (current_index))
								)
							)
							;all non-coreset cases
							(compute_on_contained_entities (query_equals ".keeping" (null)) )
						)
				))

				(if (>=
						(size (contained_entities (query_not_equals ".keeping" (null)) ))
						reduce_max_cases
					)
					(assign (assoc done .true))
				)
			)
		)

		;the list of case ids to be removed are those that did not have an iteration value assigned to ".keeping"
		(declare (assoc
			cases_to_remove (contained_entities (query_equals ".keeping" (null)) )
		))

		(if !tsTimeFeature
			(call !FilterCasesToRemoveForTimeSeries (assoc ensure_enough_to_remove .true))
		)


		(if (size cases_to_remove)
			(call !RemoveCases (assoc
				cases cases_to_remove
				distribute_weight_feature distribute_weight_feature
			))

			;else no cases to remove, clear caches for these added features so they can be cleared out quickly
			(reclaim_resources (null) .false [".keeping" ".neighbor_surprisal"])
		)

		;null out added features, TODO: 24296 - remove these labels/indices from entities altogether
		(map
			(lambda
				(assign_to_entities (current_value) (assoc
					".keeping" (null)
					".neighbor_surprisal" (null)
				))
			)
			(call !AllCases)
		)

		;if the number of cases has been reduced by 'e' or more, auto analyze if needed
		(if (< (call !GetNumTrainingCases) (/ num_cases 2.718281828459))
			(call !AutoAnalyzeIfNeeded (assoc
				skip_auto_analyze skip_auto_analyze
				;no need to compute entropies for all cases anymore since reduction is complete
				in_reduce_data .false
			))
		)

		(accum_to_entities (assoc !revision 1))
		(call !Return (assoc payload output))
	)

	;Helper method in reduce_data used to modify 'cases_to_remove' for time series datasets by
	;removing all series edge (first and last in a series) cases from 'cases_to_remove',
	;unless a remaining series will have less than 3 cases after removal, then this will instead
	;add the remainder case(s) to 'cases_to_remove' so the entire series is removed.
	;
	;parameters:
	; ensure_enough_to_remove: flag, if true checks whether enough 'cases_to_remove' have been selected (within 5% of desired amount),
	;							and if not, pads them from the core set and reruns this method
	#!FilterCasesToRemoveForTimeSeries
	(let
		(assoc
			entire_series_removal_id_queries
				;select those series identifiers where there will be less than 3 cases remaining after this removal pass
				;because these entire series should be removed at that point
				(filter
					(lambda
						(<
							(size (contained_entities
								(query_not_in_entity_list cases_to_remove)
								;(current_value) is in the format of (list (query_equals "series_feature_id" value) ... ) for all affected series ids
								(current_value)
							))
							3
						)
					)
					;a list of affected series identifier queries for this batch of 'cases'
					(call !GenerateUniqueSeriesQueries (assoc
						series_id_features (get !tsFeaturesMap "series_id_features")
						case_ids
							;of the selected cases, only keep those that were either the first or last case from a series
							(append
								(contained_entities
									(query_in_entity_list cases_to_remove)
									(query_equals ".reverse_series_index" 0)
								)
								(contained_entities
									(query_in_entity_list cases_to_remove)
									(query_equals ".series_index" 0)
								)
							)
					))
				)
		)

		(declare (assoc num_cases_to_remove (size cases_to_remove) ))

		;do not remove first (.series_index == 0) or last (.reverse_series_index == 0) cases for any series
		(assign (assoc
			cases_to_remove
				(contained_entities
					(query_in_entity_list cases_to_remove)
					(query_not_equals ".reverse_series_index" 0)
					(query_not_equals ".series_index" 0)
				)
		))

		;there were series that will need to be entirely removed, add all those series cases for removal
		(if (size entire_series_removal_id_queries)
			(accum (assoc
				cases_to_remove
					(apply "append" (map
						(lambda
							(contained_entities
								(query_not_in_entity_list cases_to_remove)
								(current_value)
							)
						)
						entire_series_removal_id_queries
					))
			))
		)

		;if the number of cases to remove has gone down by more than 5%, use the latest cases added to the core set to pad cases_to_remove
		(if ensure_enough_to_remove
			(let
				(assoc
					percent_edge_cases_filtered_out (/ (- num_cases_to_remove (size cases_to_remove)) num_cases_to_remove)
				)

				(if (> percent_edge_cases_filtered_out 0.05)
					(seq

						(accum (assoc
							cases_to_remove
								(contained_entities
									;find the latest cases added to the core set, those with the highest iteration values
									(query_max
										".keeping"
										;pad cases_to_remove with the extra 'percent_edge_cases_filtered_out', assuming that,
										;on average, that many will be filtered out again from these padded cases
										(* (- num_cases_to_remove (size cases_to_remove)) (+ 1 percent_edge_cases_filtered_out))
									)
								)
						))

						;rerun the filter, but prevent getting stuck in a loop by only doing it once more
						(call !FilterCasesToRemoveForTimeSeries (assoc ensure_enough_to_remove .false))
					)
				)
			)
		)
	)


	;helper method that merges duplicate cases during the reduce_data flow
	#!ReduceMergeDuplicateCases
	(let
		(assoc
			;assoc of case id -> duplicate neighbors (not including the case id)
			duplicate_neighbors_map {}
			duplicates []
		)
		(while (size all_duplicate_cases_map)
			;find all duplicates for a given case in this list
			(assign (assoc
				duplicates
					(contained_entities (values
						(map
							(lambda (query_equals (current_index) (current_value)) )
							;assoc of feature -> value for all the values in a duplicate case
							(zip features (retrieve_from_entity (first (indices all_duplicate_cases_map)) features))
						)
					))
			))

			;quick check for a failure case that should never occur, but if query engine somehow fails the above query to find anything,
			;ensure that the list all_duplicate_cases_map keeps shrinking to prevent being stuck in an infinite loop
			(if (= 0 (size duplicates))
				(assign (assoc
					all_duplicate_cases_map (remove all_duplicate_cases_map (first (indices all_duplicate_cases_map)) )
				))

				(seq
					(assign (assoc
						all_duplicate_cases_map (remove all_duplicate_cases_map duplicates)
					))
					(accum (assoc
						duplicate_neighbors_map (associate (first duplicates) (tail duplicates))
					))
				)
			)
		)

		;merge duplicates by iterating over the assoc of case id -> duplicate ids
		(map
			(lambda (seq
				;distribute the total weight from all the dupes to the case
				(accum_to_entities (current_index) (associate
					distribute_weight_feature
						(apply "+" (map
							(lambda (first (current_value)))
							(values (compute_on_contained_entities
								;pull the weight feature value for all the duplicates
								(query_in_entity_list (current_value 1))
								(query_exists distribute_weight_feature)
							))
						))
				))

				;remove all the duplicates
				(call !RemoveCases (assoc
					cases (current_value 1)
					;weight has already been distributed, don't do it again
					distribute_weight_feature (null)
				))
			))
			duplicate_neighbors_map
		)
	)


	;helper method which queries and returns the specified quantile of influence weight entropies
	; for use in data reduction and auto-ablation.
	; influence_weight_entropy_threshold: optional, default !autoAblationInfluenceWeightEntropyThreshold. cases with
	; 	influence weight entropy above this quantile will be removed
	; weight_feature: optional, default !autoAblationWeightFeature. name of feature whose values to use as case weights
	#!RecomputeAndCacheMaxInfluenceWeightEntropy
	(declare
		(assoc
			influence_weight_entropy_threshold !autoAblationInfluenceWeightEntropyThreshold
			weight_feature !autoAblationWeightFeature
		)

		;output quantile value if influence weight entropies exist, otherwise output
		; infinity, meaning that no cases will be ablated/reduced until this method
		; is called when there exists one or more cases with defined influence weight entropies.
		(if
			(call !HasInfluenceWeightEntropies)
			(compute_on_contained_entities
				(query_exists !internalLabelInfluenceWeightEntropy)
				(query_not_equals !internalLabelInfluenceWeightEntropy 0)
				(query_quantile
					!internalLabelInfluenceWeightEntropy
					influence_weight_entropy_threshold
					weight_feature
				)
			)
			.infinity
		)
	)

	;determine whether a new case (one that is not in the dataset) should be ablated (trained as weights only) or kept
	;returns .true if case should be kept and trained
	;returns .false when case should be ablated
	;
	;parameters:
	; features: list of features to react to
	; feature_values: list of feature values to react to
	#!ShouldNewCaseBeTrained
	(let
		(assoc
			; tuple of [ entropy, most influential case id, influence of most influential ]
			influentials_entropy_tuple
				(call !ComputeInfluenceWeightEntropyTuple (assoc
					features features
					feature_values feature_values
					output_most_influential_only .true
				))
		)

		;case can be ablated if:
		;a) it's an exact duplicate
		;b) a near-duplicate (very similar to exactly one other case)
		;c) it's not "too far" away (keep outliers and dissimilar cases)
		;d) its influence weight entropy is relatively high (above a threshold, case can be evenly distributed among its neighbors)
		(or
			;case may be kept because influentials are not evenly distributed (their entropy is low)
			(if (<= (first influentials_entropy_tuple) !autoAblationMaxInfluenceWeightEntropy)
				;check if case is a near duplicate, if it is, it can be ablated
				(if (and
						;entropy is < 0.1, meaning significantly closer to one case than any of the others
						(< (first influentials_entropy_tuple) 0.1)
						;closest case influence is in the top 10%, meaning it's very close to its one neighbor
						(> (last influentials_entropy_tuple) !autoAblationNinetyPercentQuantileInfluence)
					)
					.false

					;else keep case
					.true
				)

				;always ablate perfect matches/identical cases
				;entropy value will have been set to .infinity by !ComputeInfluenceWeightEntropyTuple for duplicates
				(= .infinity (first influentials_entropy_tuple))
				.false

				;else ablate? check distance to closest vs its distance to its K'th closest
				(let
					(assoc
						neighbor_cases_pairs
							(compute_on_contained_entities
								(query_not_in_entity_list [(get influentials_entropy_tuple 1)])
								(query_nearest_generalized_distance
									closest_k
									features
									(get influentials_entropy_tuple 1)
									p_parameter
									feature_weights
									!queryDistanceTypeMap
									query_feature_attributes_map
									feature_deviations
									(null)
									dt_parameter
									weight_feature
									(rand)
									(null) ;radius
									!numericalPrecision
									;output as ordered pair
									.true
								)
							)
					)

					;keep this training case if the 'distance' to its closest existing case is relatively large
					;i.e., if its probability (inverse distance) is too small
					(<
						;closest case probability (inverse distance)
						(last influentials_entropy_tuple)
						;closest case's farthest neighbor influence probability (inverse distance)
						(last (last neighbor_cases_pairs))
					)
				)
			)

			; This is probably unnecessary but in case we cannot compute the requested influence weight
			; quantile return .true as well (keep the case)
			(= (null) !autoAblationMaxInfluenceWeightEntropy)
		)
	)

	;Helper method called by train to see if ablation can be skipped altogether
	;outputs true if ablation can be skipped
	;parameters:
	; cases: list of cases from train method
	; num_cases: number of currently trained cases in the dataset
	#!CanTrainAblationBeSkipped
	(if !autoAblationEnabled
		(and
			(or
				(not (and
					(call !HasInfluenceWeightEntropies) !autoAblationEnabled
				))
				(< (+ num_cases (size cases)) !autoAblationMinNumCases )
			)
			(= (null)
				!autoAblationExactPredictionFeatures
				!autoAblationTolerancePredictionThresholdMap
				!autoAblationRelativePredictionThresholdMap
				!autoAblationResidualPredictionFeatures
				!autoAblationConvictionLowerThreshold
				!autoAblationConvictionUpperThreshold
			)
			;keep any cases that only have null values
			;if there are zero 'null only' cases, can skip ablation since every case should be trained
			(= 0
				(size (filter
					(lambda (and
						(= (null) (first (current_value)))
						(apply "=" (current_value))
					))
					cases
				))
			)
		)

		.true
	)

	;Helper method called by train to check whether case values are within thresholds for training or if it should be ablated
	;returns true if case should be kept / not ablated
	;return false if case should be ablated
	#!CaseOutsideThresholds
	(seq
		(if
			(= (null)
				!autoAblationExactPredictionFeatures
				!autoAblationTolerancePredictionThresholdMap
				!autoAblationRelativePredictionThresholdMap
				!autoAblationResidualPredictionFeatures
				!autoAblationConvictionLowerThreshold
				!autoAblationConvictionUpperThreshold
			)
			;return true, keep case / do not ablate
			(conclude .true)
		)

		(declare (assoc
			action_features
				(filter
					(lambda (or
						(contains_value !autoAblationExactPredictionFeatures (current_value))
						(contains_index !autoAblationTolerancePredictionThresholdMap (current_value))
						(contains_index !autoAblationRelativePredictionThresholdMap (current_value))
						(contains_value !autoAblationResidualPredictionFeatures (current_value))
					))
					features
				)
			feature_value_map (zip features feature_values)
		))

		(declare (assoc
			action_values (unzip feature_value_map action_features)
			residual_map (assoc)
		))

		(or ;evaluate in order and return the first true block

			;if the dataset is too small, skip ablatement, return true to force training
			(< (call !GetNumTrainingCases) !autoAblationMinNumCases)

			;if we have action_features, need to react first
			(if (!= (list) action_features)
				;store the reaction to reuse for both
				(let
					(assoc
						reaction_values
							(map
								(lambda (let
									(assoc
										feature (current_index 1)
										;flag set if this feature uses residual as its threshold type
										ablate_by_feature_residuals
											(contains_value !autoAblationResidualPredictionFeatures (current_index 1))
									)
									(declare (assoc
										context_features (filter (lambda (!= feature (current_value))) features)
									))

									;react to each of the action features, using all other features as contexts
									(declare (assoc
										reaction
											(call !ReactDiscriminative (assoc
												return_action_values_only .true
												context_features context_features
												context_values (unzip feature_value_map context_features)
												action_features (list feature)
												details
													(if ablate_by_feature_residuals
														(assoc "feature_full_residuals" .true)
													)
												substitute_output .false
												skip_encoding .true
												skip_decoding .true
											))
									))

									;output the predicted value
									(if ablate_by_feature_residuals
										(seq
											;store the residual min and max tolerance for this feature
											(accum (assoc
												residual_map
													(associate feature (assoc
														"min" (get reaction (list "feature_full_residuals" feature))
														"max" (get reaction (list "feature_full_residuals" feature))
													))
											))

											(first (get reaction "action_values"))
										)

										(first reaction)
									)

								))
								(zip action_features)
							)
					)

					(or
						;if we filter out any values outside of the defined thresholds (that should be trained on)
						;allow training because these values are not being predicted by the system
						(!=
							(size action_values)
							;if an action_value is same as/within threshold of prediction, it will not be filtered out
							;so if this list is the same size as the original action_values, all action values were predicted correctly
							;but if it's not the same size, the action values are different enough to be trained on
							(size (filter
								(lambda (let
									(assoc
										action_feature (current_index 1)
										action_value (get feature_value_map (current_index 1))
									)
									(declare (assoc
										threshold_type
											(if
												(contains_value !autoAblationExactPredictionFeatures action_feature)
												"exact"
												(contains_index !autoAblationTolerancePredictionThresholdMap action_feature)
												"tolerance"
												(contains_value !autoAblationResidualPredictionFeatures action_feature)
												"residual"
												(contains_index !autoAblationRelativePredictionThresholdMap action_feature)
												"relative"
												(null)
											)
									))

									(if (= threshold_type "exact")
										(= action_value (current_value))

										;for discrete or residual, set the min and max and check if actual value is outside of those bounds
										(or (= threshold_type "tolerance") (= threshold_type "residual") )
										(let
											(assoc
												min
													(if (= threshold_type "tolerance")
														(get !autoAblationTolerancePredictionThresholdMap (list action_feature 0))
														(get residual_map (list action_feature "min"))
													)
												max
													(if (= threshold_type "tolerance")
														(get !autoAblationTolerancePredictionThresholdMap (list action_feature 1))
														(get residual_map (list action_feature "max"))
													)
											)
											(and (>= (current_value) (- action_value min)) (<= (current_value) (+ action_value max)))
										)

										;PERCENT is (last threshold)
										(= threshold_type "relative")
										(<=
											(/ (abs (- (current_value) action_value)) (current_value))
											(get !autoAblationRelativePredictionThresholdMap action_feature)
										)

										;if threshold_type is explicitly undefined, treat it as outside of bounds and filter out the value
										;thus forcing this case to be trained
										(= threshold_type (null))
										.false
									)
								))
								reaction_values
							))
						)
					)
				)
			)

			;conviction thresholds
			(if
				(or (!= !autoAblationConvictionLowerThreshold (null)) (!= !autoAblationConvictionUpperThreshold (null)))
				(let
					(assoc
						conviction_value
							(get
								(call !SingleReactGroup (assoc
									features features
									new_cases (list feature_values)
									skip_encoding .true
									similarity_conviction .false
								))
								"familiarity_conviction_addition"
							)
					)

					(or
						;if both threshold values set, must be within both, otherwise check only the provided one
						(if (and (!= (null) !autoAblationConvictionLowerThreshold) (!= (null) !autoAblationConvictionUpperThreshold))
							(and (< conviction_value !autoAblationConvictionUpperThreshold) (> conviction_value !autoAblationConvictionLowerThreshold))

							(!= (null) !autoAblationConvictionLowerThreshold)
							(> conviction_value !autoAblationConvictionLowerThreshold)

							(< conviction_value !autoAblationConvictionUpperThreshold)
						)

						;if the dataset is empty, SingleReactGroup returns 0, force training
						(= conviction_value 0)
					)
				)
				;else false, ablate case
				.false
			)
		)
	)

	#!HasInfluenceWeightEntropies
	;small query which determined whether or not the cases contained within this trainee have
	; cached influence weight entropies.
	(if
		(size (contained_entities
			(query_exists !internalLabelInfluenceWeightEntropy)
			(query_not_equals !internalLabelInfluenceWeightEntropy (null))
		))
		.true
		.false
	)
)