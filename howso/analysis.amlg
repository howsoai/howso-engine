;Contains methods for hyperparameter analysis and feature deviation and weights calculations.
(null
	;automatically analyze the dataset using stored parameters from previous analyze calls
	;{long_running .true statistically_idempotent .true}
	#auto_analyze
	(let
		(assoc saved_analyze_parameters_map (retrieve_from_entity "!savedAnalyzeParameterMap"))

		;if this is called before regular analyze, analyze the dataset as targetless
		(if (= (null) saved_analyze_parameters_map)
			(call analyze (assoc context_features !trainedFeatures))

			(call analyze saved_analyze_parameters_map)
		)
	)


	;Analyzes the data to compute the appropriate statistics, uncertainties, and select parameters as appropriate.
	;{long_running .true statistically_idempotent .true}
	#analyze
	(declare
		(assoc
			;{type "list" values "string"}
			;list of context features to analyze
			context_features (list)
			;{type "list" values "string"}
			;list of action features to analyze for. applicable only to 'single_targeted' mode
			action_features (list)
			;{type "number" min 1}
			;number of cross validation folds to do. value of 1 does hold-one-out instead of k-fold
			k_folds 1
			;{type "boolean"}
			;flag, if true will not do any search over k, p, and dt parameters, but still may compute feature residuals
			; depending on other parameters.
			bypass_hyperparameter_analysis .false
			;{type "boolean"}
			;flag, if set to true will skip the computation of feature residuals
			bypass_calculate_feature_residuals .false
			;{type "boolean"}
			;flag, if set to true will skip the computation of feature weights
			bypass_calculate_feature_weights .false
			;{type ["boolean" "null"]}
			;whether to use deviations for LK metric in queries. When true forces the use
			;	of deviations, when false will not use deviations. If unspecified, the better performing option will be selected.
			use_deviations (null)
			;{type "number" min 2}
			;number of cases to use to approximate deviations and residuals for both targetless and targeted flows.
			;defaults to 1000 if unspecified.
			num_deviation_samples (null)
			;{type "number" min 2}
			;number of samples to use to compute feature probabilities, only applies to targetless flow.
			;If unspecified will be dynamically set based on number features and data characteristics.
			num_feature_probability_samples (null)
			;{type "number" min 2}
			;number of cases to sample used for grid search for the targeted flow. only applies for k_folds = 1. defaults to 1000.
			num_analysis_samples (null)
			;{type "number" min 2}
			;number of samples to use for analysis. the rest will be randomly held-out and not included in calculations
			analysis_sub_model_size (null)
			;{type "list" values ["number" "list"]}
			;list of values for k (number of cases making up the local space) to grid search during analysis
			;if a value is a list of values, treats the inner list as a tuple of: influence cutoff percentage, min K, max K and extra K.
			k_values (null)
			;{type "list" values "number"}
			;list of values for p (the parameter of the Lebesgue space) to grid search during analysis
			p_values (null)
			;{type "list" values ["number" "string"]}
			;list of values for dt (the distance transform) to grid search during analysis
			dt_values (null)
			;{type "string" enum ["single_targeted" "omni_targeted" "targetless"]}
			;enumeration, default is "single_targeted"
			;   "single_targeted" = analyze hyperparameters for the specified action_features
			;   "omni_targeted" = analyze hyperparameters for each action feature, using all other features as context_features. if action_features aren't specified, uses all context_features.
			;   "targetless" = analyze hyperparameters for all context features as possible action features, ignores action_features parameter
			targeted_model "single_targeted"
			;{type "string"}
			;name of feature whose values to use as case weights
			weight_feature ".case_weight"
			;{ref "UseCaseWeights"}
			;when true will scale influence weights by each case's weight_feature weight. if use_case_weights isn't specified, it will
			;	be true if auto ablation is enabled and false otherwise
			use_case_weights (null)
			;{type "list" values "string"}
			;list of features whose values to use to rebalance case weighting of the data and to store into weight_feature
			rebalance_features (null)

			;{type "boolean"}
			;when true, the default, will compute and use a sparse deviation matrix (SDM) for each nominal feature in all similarity queries.
			;Enabling SDM will typically incur a small to moderate penalty on speed when using nominal features in inference in exchange for
			;yielding higher quality inference. The magnitude of the changes are dependent on relationships among the data and the task at hand.
			use_sdm .true
			;{type "boolean"}
			;when true, a subtrainee will be created that will be used during queries to approximate deviations of the data around the query.
			; This incurs a penalty in speed, but can improve performance in long tailed data, especially when using targetless parameters.
			use_dynamic_deviations .false
			;{type "number" min 2}
			;the number of cases to store within the subtrainee that is used to the deviations of the data around predictions
			dynamic_deviations_subtrainee_size 500
			;{type "number" min 0}
			;Percent threshold used to dynamically limit the number of samples used to determine feature probabilities, defaults to 0.25%
			;When set to 0 will use all num_feature_probability_samples
			convergence_threshold 0.0025
			;{type "number" min 1}
			;Rate of increasing the size of each subsequent sample used to dynamically limit the total number of samples used to determine feature probabilities.
			;defaults to a rate of 1.05, increasing by 5% until the delta between residuals is less than 'convergence_threshold'.
			convergence_samples_growth_rate 1.05
			;{type "number" min 1}
			;The minumum size of the first batch of cases used when dynamically sampling robust residuals used to determine feature probabilities
			;	Default of 5000
			convergence_min_size 5000
			;{type "boolean"}
			;when true, used by reduce_data flow to simplify analyze flow by skipping computation of feature weights
			reduce_only .false
		)

		(call !ValidateParameters)
		(call !ValidateFeatures)

		(if (and
				(size action_features)
				(= (size context_features) 0)
			)
			(conclude
				(call !Return (assoc
					errors ["`context_features` must be specified when `action_features` is specified."]
				))
			)
		)

		;analyze uses its own explicit warnings since it should not output warnings from all the
		;react calls made during the analyze flow
		(declare (assoc analyze_warnings (assoc) ))

		(if (and
				(= (null) use_case_weights)
				!hasPopulatedCaseWeight
				(= weight_feature ".case_weight")
			)
			(assign (assoc use_case_weights .true ))
		)

		(if (or use_case_weights (size rebalance_features))
			;CreateCaseWeights will find all cases missing weight_feature, then initialize
			;weight_feature with a value of 1.0 for each
			(call !CreateCaseWeights (assoc
				feature_name weight_feature
				has_rebalance_features (!= 0 (size rebalance_features))
			))
		)

		;if k_folds is not a number value, or it's less than 1, ensure that it's overwritten with the default value
		(if (or (not (= "number" (get_type_string k_folds))) (< k_folds 1))
			(assign (assoc k_folds 1))
		)

		(declare (assoc num_cases (call !GetNumTrainingCases)))

		;cap max k_folds to the number of cases in the dataset
		(if (> k_folds num_cases)
			(assign (assoc k_folds num_cases))
		)

		(declare (assoc holdout_entity_name (null) ))

		;if user specified how many samples to use for analysis, randomly hold out the rest
		(if (!= (null) analysis_sub_model_size)
			(let
				(assoc dataset_size (call !GetNumTrainingCases))

				;analysis_sub_model_size must be at least 2 * number of k_folds otherwise there may be no cases in a fold
				(if (< analysis_sub_model_size (* 2 k_folds))
					(assign (assoc analysis_sub_model_size (* 2 k_folds)))
				)

				(if (> dataset_size analysis_sub_model_size)
					(assign (assoc
						holdout_entity_name (call !HoldOutRandomCases (assoc num_samples (- dataset_size analysis_sub_model_size)))
					))
				)
			)
		)

		;if no action features were specified, and neither was targeted_model, set it to targetless
		(if (and
				(= 0 (size action_features))
				(or
					(= (null) targeted_model)
					(= "single_targeted" targeted_model)
				)
			)
			(assign (assoc targeted_model "targetless"))

			;else one or more action features specified but targeted_model wasn't, set it to single_targeted
			(and
				(>= 1 (size action_features))
				(= (null) targeted_model)
			)
			(assign (assoc targeted_model "single_targeted"))

			;if omni_targeted and didn't provide action_features, assume all features are action features
			(and
				(= "omni_targeted" targeted_model)
				(= 0 (size action_features))
			)
			(assign (assoc action_features context_features))

			;default is targetless if inputs are bad
			(not (contains_value (list "targetless" "single_targeted" "omni_targeted") targeted_model))
			(assign (assoc targeted_model "targetless"))
		)

		;if this is a time series Trainee, there are no targetless hyperparameters,
		; and this analyze is targeted, emit a warning to the user that recommends
		; a targeted analyze
		(if (and
				(!= (null) !tsTimeFeature)
				(=
					0
					;the number of saved hyperparameter maps that are targetless
					(size (filter (lambda (= "targetless" (first (current_value))) ) !hyperparameterParamPaths))
				)
				(!= targeted_model "targetless")
			)
			(accum (assoc
				analyze_warnings
					(associate (concat
						"It is recommended to use a \"targetless\" analysis of the data for a time-series Trainee. "
						"Please analyze the data once more with no action features specified and the value \"targetless\" "
						"specified for the \"targeted_model\" parameter."
					))
			))
		)

		;save the parameters that analyze was called with
		(assign_to_entities (assoc
			!savedAnalyzeParameterMap
				(assoc
					"context_features" context_features
					"action_features" action_features
					"k_folds" k_folds
					"bypass_hyperparameter_analysis" bypass_hyperparameter_analysis
					"bypass_calculate_feature_residuals" bypass_calculate_feature_residuals
					"bypass_calculate_feature_weights" bypass_calculate_feature_weights
					"use_deviations" use_deviations
					"num_deviation_samples" num_deviation_samples
					"k_values" k_values
					"p_values" p_values
					"dt_values" dt_values
					"targeted_model" targeted_model
					"num_analysis_samples" num_analysis_samples
					"num_feature_probability_samples" num_feature_probability_samples
					"analysis_sub_model_size" analysis_sub_model_size
					"use_case_weights" use_case_weights
					"weight_feature" weight_feature
					"rebalance_features" rebalance_features
					"use_sdm" use_sdm
					"convergence_threshold" convergence_threshold
					"convergence_samples_growth_rate" convergence_samples_growth_rate
					"convergence_min_size" convergence_min_size
					"reduce_only" reduce_only
				)
		))

		;if the bypass hyperparameter analysis flag is null or is false, analyze hyper parameters
		(if (not bypass_hyperparameter_analysis)
			(call !Analyze (assoc
				context_features context_features
				action_features action_features
				k_folds k_folds
				k_values k_values
				p_values p_values
				dt_values dt_values
				use_deviations use_deviations
				targeted_model targeted_model
				num_analysis_samples num_analysis_samples
				num_feature_probability_samples num_feature_probability_samples
				num_deviation_samples (if (> num_deviation_samples 0) num_deviation_samples 1000)
				use_case_weights use_case_weights
				weight_feature weight_feature
				use_dynamic_deviations use_dynamic_deviations
				use_sdm use_sdm
				reduce_only reduce_only
			))
		)

		;only calculate feature residuals if they aren't being bypassed and haven't been calculated above
		(if (and
				(= .false bypass_calculate_feature_residuals)
				(or bypass_hyperparameter_analysis (!= "targetless" targeted_model))
			)
			(let
				(assoc
					;remove possible duplicates from all_features while mantaining order of features
					all_features (values (append context_features action_features) .true)
				)

				;for targetless flows, there are no action features, so use (null) as the hyperparameter action feature
				(if (= "targetless" targeted_model)
					(assign (assoc action_features [(null)] ))
				)

				(map
					(lambda
						(let
							(assoc
								hyperparam_map
									(call !CalculateResidualsAndUpdateParameters (assoc
										context_features all_features
										;default the num_samples to 1000
										num_samples (if (> num_deviation_samples 0) num_deviation_samples 1000)
										robust_residuals (= "targetless" targeted_mode)
										;use the specific action feature's hyperparameters
										hyperparameter_feature (current_value 2)
										weight_feature weight_feature
										use_case_weights use_case_weights
										compute_null_uncertainties .false
										use_shared_deviations .true
									))
							)

							(if (= (first (get hyperparam_map "paramPath")) ".default")
								;updating default hyperparam set with featuredeviations
								(assign_to_entities (assoc !defaultHyperparameters hyperparam_map) )

								;update the appropriate hyperparameter set
								(assign_to_entities (assoc !hyperparameterMetadataMap (set !hyperparameterMetadataMap (get hyperparam_map "paramPath") hyperparam_map)) )
							)
						)
					)
					action_features
				)
			)
		)

		(if holdout_entity_name
			(call !RestoreHeldOutCases (assoc holdout_entity_name holdout_entity_name))
		)

		(accum_to_entities (assoc !revision 1))

		(call !Return (assoc warnings (if (size analyze_warnings) (indices analyze_warnings)) ))
	)

	;helper method to run the targeted or targetless analyze
	#!Analyze
	(let
		(assoc
			;assoc of case id -> (assoc ".imputed" (list of imputed features))
			imputed_cases_map (compute_on_contained_entities (query_exists !internalLabelImputed))
		)

		;prime caches and update inactive features
		(if !inactiveFeaturesNeedCaching (call !UpdateInactiveFeatures))

		;if any action feature is inactive, do targetless analyze instead
		(declare (assoc
			inactive_action_feature
				(first (filter
					(lambda (contains_index !inactiveFeaturesMap (current_value)))
					action_features
				))
		))
		(if inactive_action_feature
			(seq
				(assign (assoc
					targeted_model "targetless"
					context_features (values (append context_features action_features) .true)
					analyze_warnings
						(associate
							(concat "There are no values to analyze for '" inactive_action_feature "', defaulting to 'targetless'.") (null)
						)
				))
				(assign (assoc action_features [] ))
			)
		)

		(if (!= "omni_targeted" targeted_model)
			(if (or (= "targetless" targeted_model) (= 1 (size action_features)))
				(call !AnalyzeHyperparameters (assoc
					targeted_model targeted_model
					context_features context_features
					action_feature (if (!= "targetless" targeted_model) (first action_features))
					num_deviation_samples num_deviation_samples
					num_analysis_samples num_analysis_samples
					num_feature_probability_samples num_feature_probability_samples
				))

				;else single_targeted but with multiple action features, all feature analyze iterations should be 'robust' until the last one which is 'full'
				(map
					(lambda (let
						(assoc
							accumulated_context_features context_features
							action_feature (current_value 1)
						)

						(call !AnalyzeHyperparameters (assoc
							targeted_model "single_targeted"
							context_features accumulated_context_features
							action_feature action_feature
							num_deviation_samples num_deviation_samples
						))

						;accumulate the action feature to the contexts for the next iteration
						(accum (assoc accumulated_context_features action_feature))
					))
					action_features
				)
			)

			;else for omni-targeted, loop over every action feature and analyze each one as 'single_targeted', 'full'
			(map
				(lambda (let
					(assoc
						filtered_context_features (filter (lambda (!= (current_value) (current_value 2))) context_features)
						action_feature (current_value 1)
					)

					(call !AnalyzeHyperparameters (assoc
						targeted_model "single_targeted"
						context_features filtered_context_features
						action_feature action_feature
						num_deviation_samples num_deviation_samples
					))
				))
				action_features
			)
		)

		;compute rebalance features (e.g., id-based) case weights
		(if (size rebalance_features)
			(call !ComputeAndStoreRebalanceFeatureCaseWeights (assoc rebalance_features rebalance_features))

			;else clear cached rebalanced features
			(assign_to_entities (assoc
				!continuousRebalanceFeatures (null)
				!nominalRebalanceFeatures (null)
				!cachedRebalanceClassValueWeightMap (null)
				!cachedRebalanceUnknownValueWeightMap (null)
				!cachedRebalanceTotalMass (null)
				!cachedTotalMass (null)
			))
		)

		;reimpute features after analysis and overwrite the values with the newly imputed ones
		(if (size imputed_cases_map)
			(let
				(assoc
					;use all the unique context and action features for imputation, or !trainedFeatures if none were provided
					analyze_features
						(values
							(append (if (= 0 (size context_features)) !trainedFeatures context_features) action_features)
							.true
						)
				)
				||(map
					(lambda (let
						(assoc
							impute_case_id (current_index 1)
							imputed_features (get (current_value 1) !internalLabelImputed)
							;assoc of all the feature values for this case
							case_feature_value_map (zip analyze_features (retrieve_from_entity (current_index 1) analyze_features) )
						)
						(declare (assoc
							imputed_values
								(map
									(lambda (let
										(assoc impute_feature (current_value 1))
										;don't use the imputed feature in the context for this react, use all others
										(declare (assoc
											impute_context_features (indices (remove case_feature_value_map impute_feature))
										))
										(first
											(call !ReactDiscriminative (assoc
												context_features impute_context_features
												context_values (unzip case_feature_value_map impute_context_features)
												action_features (list impute_feature)
												ignore_case impute_case_id
												;don't encode the input because it's being passed in directly from the case
												skip_encoding .true
												;don't decode because the output will be stored as-is right afterwards
												skip_decoding .true
												return_action_values_only .true
												allow_nulls .false
												impute_react .true
											))
										)
									))
									imputed_features
								)
						))

						;overwrite the feature values in this case with the new imputed_values
						(assign_to_entities impute_case_id (zip imputed_features imputed_values))
					))
					imputed_cases_map
				)

				(call !UpdateHasNulls (assoc features analyze_features))
			)
		)

		(if (and !tsTimeFeature !tsTimeFeatureUniversal)
			(call !UpdateMinimumTimeBound (assoc
				hyperparam_map (call !GetHyperparameters (assoc feature (null) ))
			))
		)

		;if there's a !computedFeaturesMap call react_into_features to recompute them,
		;only if not being called from react_into_features to prevent infinite loop
		(if (and !computedFeaturesMap (not from_react_into_features))
			(call react_into_features (append
				(get !computedFeaturesMap "computed_map")
				{
					features (get !computedFeaturesMap "context_features")
					analyze .false
					overwrite .true
				}
				(if (get !computedFeaturesMap "weight_feature")
					(assoc
						"use_case_weights" .true
						"weight_feature" (get !computedFeaturesMap "weight_feature")
					)
					{}
				)
			))
		)

		;if auto ablation is enabled or the user has specifically requested to cache influence weight entropies
		;recompute and cache influence weight entropies here at the end of the analyze flow
		(if
			!autoAblationEnabled
			(seq
				(call !InitializeAutoAblation)

				(declare  (assoc
					hyperparam_map
						(call !GetHyperparameters (assoc
							context_features features
							weight_feature weight_feature
						))
				))
				(declare (assoc
					closest_k (get hyperparam_map "k")
					p_parameter (get hyperparam_map "p")
					dt_parameter (get hyperparam_map "dt")
					feature_weights (get hyperparam_map "featureWeights")
					feature_deviations (get hyperparam_map "featureDeviations")
					query_feature_attributes_map (get hyperparam_map "featureDomainAttributes")
				))

				(call !ComputeAndStoreInfluenceWeightEntropies (assoc
					features
						(if (= 0 (size context_features))
							!trainedFeatures
							context_features
						)
					use_case_weights .true
					weight_feature weight_feature

					;if in reduce_data flow, compute_all set to true to ensure all cases have the value stored for proper data reduction
					compute_all in_reduce_data
				))
			)
		)
	)

	;called on none or one action feature at a time by #Analyze
	; targeted_model: enumeration, default is "single_targeted"
	;   "single_targeted" = analyze hyperparameters for the specified action_features
	;   "targetless" = analyze hyperparameters for all context features as possible action features, ignores action_features parameter
	; context_features: list of context features to analyze for
	; action_features: list of action features to analyze for
	; num_analysis_samples: optional. number of cases to sample during analysis. only applies for k_folds = 1
	; num_deviation_samples: optional. initial number of samples to use for computing residuals
	; derived_auto_analyzed: optional, if true, will set the 'derivedAutoAnalyzed' parameter to true in the hyperparameter set,
	;			denoting that these parameters were auto-analyzed during derive features flow.
	#!AnalyzeHyperparameters
	(declare
		(assoc
			targeted_model "targetless"
			context_features (list)
			action_feature (null)
			num_analysis_samples (null)
			num_deviation_samples 1000
			num_feature_probability_samples (null)
		)

		(declare (assoc
			num_cases (call !GetNumTrainingCases)
		))
		;can't analyze 1 or 0 cases
		(if (< num_cases 2) (conclude 1) )

		;analyze one action feature at a time (for targeted only)
		(if (!= (null) action_feature)
			(assign (assoc action_features [action_feature] ))
		)

		(declare (assoc
			grid_search_error (null)
			analyzed_hp_map (null)
			previous_analyzed_hp_map (null)
			baseline_hyperparameter_map (null)
			initial_residual_values_map (null)
			context_features_key (null)
			;flag set to true if num_analysis_samples was passed in
			user_specified_num_samples (!= (null) num_analysis_samples)

			;name of deviation subtrainee, will be assigned if subtrainee is stored
			deviation_subtrainee_name (null)
		))
		(call !InitAnalyze)

		(call !UpdateHyperparameters (assoc
			use_weights .false
			use_deviations .false
		))

		(if (= "targetless" targeted_model)
			(conclude (seq
				(call !ConvergeTargetless)

				;IRW always needs subtrainee
				(if (and use_dynamic_deviations (not !useDynamicDeviationsDuringAnalyze))
					; store feature residuals for a collection of cases at end, if it wasn't part of iterative process
					(call !StoreResidualSubTrainee (assoc custom_hyperparam_map baseline_hyperparameter_map) )
				)

				(call !SetHyperparameters)
			))
		)

		(call !GridSearch)

		(call !ComputeAndUseWeights)
		(call !TestAccuracyAndKeepOrRevertHyperparameters)  ;w or w/o weights

		(if (= .false use_deviations) (conclude (call !SetHyperparameters)) )

		(declare (assoc non_deviation_hp_map baseline_hyperparameter_map))

		;deviations, no weights
		(call !UpdateHyperparameters (assoc
			use_weights .false
			use_deviations .true
			feature_deviations (call !ComputeInitialDeviations)
		))
		(call !ConvergeResiduals (assoc use_deviations .true))
		(call !GridSearch)

		;now with weights
		(call !ComputeAndUseWeights)
		(call !TestAccuracyAndKeepOrRevertHyperparameters) ;deviations w or w/o weights

		;if use of deviations should be auto determined, do that here, compare to non_deviation hyperparameters
		;and revert if to non-deviation if those were as good or better
		(if (= (null) use_deviations)
			(if (<= (get non_deviation_hp_map "gridSearchError") (get baseline_hyperparameter_map "gridSearchError"))
				(assign (assoc baseline_hyperparameter_map non_deviation_hp_map))

				;residuals were converged using non_deviation parameters, if the currently analyzed k or p are different
				;we re-converge the residuals using these updated p and k
				(or
					(!= (get non_deviation_hp_map "p") (get baseline_hyperparameter_map "p"))
					(!= (get non_deviation_hp_map "k") (get baseline_hyperparameter_map "k"))
				)
				(call !ConvergeResiduals)
			)
		)

		(if (and
				use_dynamic_deviations
				;need the subtrainee if using deviations
				(get baseline_hyperparameter_map "featureDeviations")
				(not !useDynamicDeviationsDuringAnalyze)
			)
			; store feature residuals for a collection of cases at end, if it wasn't part of iterative process
			(call !StoreResidualSubTrainee (assoc custom_hyperparam_map baseline_hyperparameter_map) )
		)

		(call !SetHyperparameters)
	)

	;helper method for Analyze that stores a subtrainee of case residuals for each feature
	;parameters:
	;	custom_hyperparam_map: the hyperparam map within analyze that should be used to run the residuals
	;	num_samples: the number of cases to put in the residual subtrainee, default 500
	;assigns to:
	;	deviation_subtrainee_name: this variable will store the name of the subtrainee, this name should be stored in the
	;					resulting hyperparameter assoc under the key "subtraineeName"
	#!StoreResidualSubTrainee
	(declare
		(assoc
			custom_hyperparam_map (null)
			num_samples dynamic_deviations_subtrainee_size

			;not parameters, variables for Residuals code
			case_ids (null)
			features
				;filter the context features to get just continuous
				(filter
					(lambda (= "continuous" (get !featureAttributes [(current_value 1) "type"])))
					context_features
				)
			k_parameter (null)
			p_parameter (null)
			dt_parameter (null)
			query_feature_attributes_map (null)
			case_features (null)
			num_context_features 0
			ordinal_features_map !ordinalFeaturesSet
			hyperparam_map (assoc)
			residuals_map (assoc)
			ordinal_residuals_map (assoc)
			;assoc of features to skip computing non-robust residuals for if they don't have enough non-null values
			skip_features_map (assoc)
			;list, length of features, each item will be a list of residuals, one per case
			feature_residuals_lists (list)

			num_training_cases (call !GetNumTrainingCases)
			valid_weight_feature .false

			features_with_nulls (list)
			null_feature_cases_map (assoc)
		)

		(call !InitResiduals)

		(assign (assoc in_analyze .true ))

		(if (= targeted_model "targetless")
			(call !ComputeDeviations)

			;targeted runs full residuals to match targeted flow
			(call !RunFullResiduals)
		)

		(assign (assoc
			deviation_subtrainee_name
				(if (contains_index custom_hyperparam_map "subtraineeName")
					(get custom_hyperparam_map "subtraineeName")

					(concat "ResidualSubTrainee" (round (* 100000 (rand)) (null) 0))
				)
		))

		;delete the previous subtrainee, might be smarter to just remove cases in the future
		(if (contains_index (call !GetHierarchy) deviation_subtrainee_name)
			(call delete_subtrainee (assoc path [deviation_subtrainee_name] ))
		)

		(call create_subtrainee (assoc
			path [deviation_subtrainee_name]
			child_id deviation_subtrainee_name
		))

		(set_entity_permissions [!traineeContainer deviation_subtrainee_name] .true)

		;add a method to remove !ValidateParameters
		(accum_entity_roots [!traineeContainer deviation_subtrainee_name]
			(zip_labels
				(list "removeValidateParameters")
				(list (lambda
					(assign_to_entities
						(assoc
							"!ValidateParameters" (null)
							"removeValidateParameters" (null)
						)
					)
				))
			)
		)
		;remove the methods
		(call_entity [!traineeContainer deviation_subtrainee_name] "removeValidateParameters")

		(call_entity [!traineeContainer deviation_subtrainee_name] "set_feature_attributes" (assoc
			feature_attributes !featureAttributes
		))

		(call_entity [!traineeContainer deviation_subtrainee_name] "train" (assoc
			features
				;a list of feature names and feature residual names
				(append
					context_features
					(map
						(lambda
							(concat "." (current_value) "_residual")
						)
						features
					)
				)
			cases
				;list of [case values and feature residuals]
				(map
					(lambda
						(append
							;case values
							(retrieve_from_entity (current_value) context_features)
							;residuals
							(map
								(lambda
									;get the feature index, case_index
									(get feature_residuals_lists [(current_index 1) (current_index 2)])
								)
								features
							)
						)
					)
					case_ids
				)
			allow_training_reserved_features .true
		))

		;and analyze the subtrainee
		(call_entity [!traineeContainer deviation_subtrainee_name] "analyze")
	)

	;initialize variables and cache feature expected values
	#!InitAnalyze
	(seq
		(if (!= (null) action_feature)
			;DiscriminativeReact will use context values for action features if provided
			;so the action feature cannot also be a context feature, otherwise predictions will
			;always be correct in analyze, leading to errors of zero.
			;this is, until TODO 17214: is complete, which adds override contexts flag to react
			(assign (assoc
				context_features (filter (lambda (!= action_feature (current_value))) context_features)
			))
		)

		;if there are computed features (e.g., 'similarity_conviction') append them to context_features
		(if !computedFeaturesMap
			(assign (assoc
				context_features
					(append
						(if (= 0 (size context_features)) !trainedFeatures context_features)
						(get !computedFeaturesMap "computed_features")
					)
			))
		)

		;if analyze is being called without any context features, default them to the full set of all features
		;targeted_model will already have be set to 'targetless' if no features are passed in
		(if (= 0 (size context_features))
			(assign (assoc
				context_features !trainedFeatures
				context_features_key !trainedFeaturesContextKey
			))

			;else build custom context feature key
			(assign (assoc context_features_key (call !BuildContextFeaturesKey (assoc context_features context_features)) ))
		)

		;if auto ablation is on and the user has not overridden the default, set use_case_weights to true
		(if (and !autoAblationEnabled (!= .false use_case_weights) )
			(assign (assoc use_case_weights .true) )
			;otherwise, if the user hasn't overridden the default, set use_case_weights to be false
			; to ensure this change works smoothly throughout the rest of the engine
			(= (null) use_case_weights)
			(assign (assoc use_case_weights .false) )
		)

		;if user doesn't want to use case weights, change weight_feature to '.none'
		(if (not use_case_weights)
			;if rebalance features are specified, it implies that case weight should be used
			(if (size rebalance_features)
				(assign (assoc use_case_weights .true ))

				(assign (assoc weight_feature ".none"))
			)

			;if there are no id features and user didn't specify a custom weight feature, treat as without case weight
			(if (and (not !hasPopulatedCaseWeight) (= weight_feature ".case_weight"))
				(seq
					(assign_to_entities (assoc !hasPopulatedCaseWeight .true))
					(call !CreateCaseWeights (assoc
						feature_name weight_feature
						has_rebalance_features (!= 0 (size rebalance_features))
					))
				)
			)
		)

		;This is being called from react_into_features
		(if (and !computedFeaturesMap from_react_into_features)
			;remove old hyperparameters for this feature set and from paths if they are found
			(if (= 0 (size !hyperparameterParamPaths))
				(assign_to_entities (assoc
					!hyperparameterMetadataMap { }
					!hyperparameterParamPaths []
				))

				(let
					(assoc
						orig_context_features (trunc context_features (- (size (get !computedFeaturesMap "computed_features"))) )
					)
					(declare (assoc
						orig_context_path
							(append
								targeted_model
								(call !BuildContextFeaturesKey (assoc context_features orig_context_features))
								weight_feature
							)
					))
					(if (contains_value !hyperparameterParamPaths orig_context_path)
						(let
							(assoc
								;remove the case weight keyed parameters
								cleaned_hp_map
									(set !hyperparameterMetadataMap (trunc orig_context_path)
										(remove
											(get !hyperparameterMetadataMap (trunc orig_context_path))
											(last orig_context_path)
										)
									)
							)

							;if the remaining context path is empty, remove it altogether at the target mode
							(if (and
									(= 1 (size (get cleaned_hp_map (first orig_context_path))))
									(= 0 (size (get cleaned_hp_map (trunc orig_context_path))))
								)
								(assign (assoc cleaned_hp_map (remove cleaned_hp_map (first orig_context_path)) ))

								;else remove only the middle context key node, leaving the target mode
								(= 0 (size (get cleaned_hp_map (trunc orig_context_path))))
								(assign (assoc
									cleaned_hp_map
										(set cleaned_hp_map (first orig_context_path)
											(remove
												(get cleaned_hp_map (first orig_context_path))
												(get orig_context_path 1)
											)
										)
								))
							)

							(assign_to_entities (assoc
								!hyperparameterMetadataMap cleaned_hp_map
								!hyperparameterParamPaths (filter (lambda (!= (current_value) orig_context_path)) !hyperparameterParamPaths)
							))
						)
					)
				)
			)
		)

		(assign (assoc
			baseline_hyperparameter_map (replace (retrieve_from_entity "!defaultHyperparameters"))
		))

		;if using default targetless hyperparameters, set p to 1 as the starting p value and k as dynamic with 0.05 cutoff
		(if (and
				(= "targetless" targeted_model)
				(= [".default"] (get baseline_hyperparameter_map "paramPath"))
			)
			(assign (assoc
				baseline_hyperparameter_map
					(set
						baseline_hyperparameter_map
						"k" (if (size k_values) (first k_values) 20) ;[0.15 2 20])
						"p" 1
						"dt" "surprisal" ;"surprisal_to_prob"
					)
			))
		)

		;store the paramPath for this hyperparameter set so it has a reference to where it's stored in !hyperparameterMetadataMap
		(accum (assoc
			baseline_hyperparameter_map
				(assoc
					"paramPath"
						(append
							(if action_feature ["targeted" action_feature] ["targetless"])
							context_features_key
							weight_feature
						)
				)
		))

		;check if auto analysis is enabled and the analysis threshold should be increased
		(if (and
				!autoAnalyzeEnabled
				(>= !dataMassChangeSinceLastAnalyze !autoAnalyzeThreshold)
			)
			(let
				(assoc
					total_case_mass
						(if use_case_weights
							(compute_on_contained_entities
								(query_not_equals weight_feature (null))
								(query_sum weight_feature)
							)

							;else it's 1 per case, thus the total mass is num_cases
							num_cases
					)
				)

				(assign_to_entities (assoc
					!autoAnalyzeThreshold
						;!autoAnalyzeThreshold is a delta from the previous threshold.
						;if several case deletions cause the above formula to return < 0, default
						; to the default !autoAnalyzeThreshold.
						(max
							(- (* total_case_mass !autoAnalyzeGrowthFactorAmount) !autoAnalyzeThreshold)
							100
						)
				))
			)
		)

		;reset !dataMassChangeSinceLastAnalyze since we are now analyzing
		(assign_to_entities (assoc !dataMassChangeSinceLastAnalyze 0.0 ))

		;for k_folds = 1, set the default num_analysis_samples to be 1000 for targeted flows
		(if (and (= 1 k_folds) (= (null) num_analysis_samples ))
			(assign (assoc num_analysis_samples 1000 ))
		)

		;10000 * (1-1/e) * num_features, cap it at 150k
		(if (= (null) num_feature_probability_samples)
			(assign (assoc
				num_feature_probability_samples (min (* 6321 (size context_features)) 150000)
			))
		)

		;pre-compute and cache all the expected feature values and nominal probabilities so that they don't have to be
		;lazy computed later during residual computations
		(assign_to_entities (assoc
			!expectedValuesMap (assoc)
			!nominalClassProbabilitiesMap (assoc)
			!cachedFeatureMinResidualMap (assoc)
			!cachedFeatureHalfMinGapMap (assoc)
			!featureNullRatiosMap (assoc)
		))

		(call !CacheExpectedValuesAndProbabilities (assoc
			features (values (append context_features action_features) .true)
			weight_feature weight_feature
			use_case_weights use_case_weights
		))

		(if (and !tsTimeFeature !tsTimeFeatureUniversal)
			(call !UpdateMinimumTimeBound (assoc
				hyperparam_map baseline_hyperparameter_map
			))
		)
	)

	;updates baseline_hyperparameter_map
	;wrapper method to grid searches hyperparameters and backs up the results into previous_analyzed_hp_map
	#!GridSearch
	(seq
		(set_rand_seed sampling_random_seed)
		(assign (assoc
			analyzed_hp_map
				(call !ComputeResidualsAcrossParametersAndSelectOptimal (assoc
					context_features context_features
					action_features action_features
					k_folds k_folds
					k_values k_values
					p_values p_values
					dt_values dt_values
					num_analysis_samples num_analysis_samples
					baseline_hyperparameter_map baseline_hyperparameter_map
				))
		))
		(call !BackupAnalyzedHyperparameters)
	)

	;autotunes the dataset to use the 'best' hyperparameters using grid-search (with optional k-fold cross validation)
	;outputs updated hyperparameter map
	;parameters:
	; context_features: list of context features
	; action_features: list of action features for action inputs
	; k_folds: number of cross validation folds to do. value of 1 does hold-one-out instead of k-fold
	; k_values : optional list of k values to grid search, if null will use default list.
	; p_values : optional list of p values to grid search, if null will use default list.
	; dt_values : optional list of distance transform values to grid search.
	;		Can specify "surprisal_to_prob" as a valid distance transform value to use surprisal in the transform. if null will use -1.
	; use_k_values: optional flag default true. if false, will use k value specified in 'baseline_hyperparameter_map'
	; use_p_values: optional flag default true. if false, will use p value specified in 'baseline_hyperparameter_map'
	; use_dt_values: optional flag default true. if false, will use dt value specified in 'baseline_hyperparameter_map'
	; p_param_categorical_mean : optional parameter to specify the lp space for calculating the mean of Mean Absolute Errors for categorical features
	; p_param_accuracy_mean : optional parameter to specify the lp space for calculating the mean of Mean Absolute Errors among all features
	; num_analysis_samples : optional. number of cases to sample during analysis. only applies for k_folds = 1
	; baseline_hyperparameter_map : the base hyperparameters to use for computation
	#!ComputeResidualsAcrossParametersAndSelectOptimal
	(declare
		(assoc
			action_features (list)
			context_features (list)
			k_folds 1
			k_values (null)
			p_values (null)
			dt_values (null)
			use_k_values .true
			use_p_values .true
			use_dt_values .true
			p_param_categorical_mean 1
			p_param_accuracy_mean 0.2
			num_analysis_samples (null)
			baseline_hyperparameter_map (assoc)
		)

		;can't do analysis if there is a max of one context feature provided and no different action features
		(if (and
				(< (size context_features) 2)
				(or
					;no action features, or the one context feature is also the action feature
					(= (size action_features) 0)
					(= context_features action_features)
				)
			)
			;return valid existing hyperparameters
			(conclude baseline_hyperparameter_map)
		)

		(if (= (null) k_values)
			(assign (assoc
				;grid search fibonacci sequence for single targeted
				k_values [3 5 8 13 21 34 55 89 144]
			))
		)
		(if (= (null) p_values)
			(assign (assoc
				p_values [0.1 0.5 1 2]
			))
		)

		;if dt is null, default it to -1, if it's empty list set it to the recommended default search list
		(if (= (null) dt_values )
			(assign (assoc
				dt_values [-1]
			))

			(= (list) dt_values)
			(assign (assoc dt_values (list -8 -2 -1 -0.5 0)))
		)

		;if the dataset is smaller than the max K value, reduce the possible k_values search
		(declare (assoc
			smallest_k (first (sort k_values))
		))
		(if (<= num_cases (apply "max" k_values))
			(assign (assoc k_values (filter (lambda (< (current_value) num_cases)) k_values)))
		)
		;if there are no valid k_values because they have all been filtered out, use the smallest one
		(if (= (list) k_values)
			(assign (assoc k_values (list smallest_k)))
		)

		;ensure the k_values are sorted largest to smallest,(ie there's a knn cache internally, when the largest K value is processed and cached
		;subsequent calls with smaller Ks will be fast)
		(assign (assoc k_values (sort .false k_values)))

		(if (not use_k_values)
			(assign (assoc k_values (list (get baseline_hyperparameter_map "k"))))
		)
		(if (not use_p_values)
			(assign (assoc p_values (list (get baseline_hyperparameter_map "p"))))
		)
		(if (not use_dt_values)
			(assign (assoc dt_values (list (get baseline_hyperparameter_map "dt"))))
		)

		(declare (assoc
			;a map of   { k_p_dt : {  k: K, p: P, dt: DT, dist: [ ...] } }
			accumulated_results_map (assoc)
			p_k_dt_key (null)
			;if k-folds == 1 and num_analysis_samples is provided, randomly sample those cases here
			sample_case_ids
				(if (!= (null) num_analysis_samples)
					(call !AllCases (assoc num num_analysis_samples rand_seed (rand)))
					(null)
				)
			best_accuracy_distance .infinity
			best_params_key (null)
			output_map (assoc)
		))

		;grid search over the P, K and DT values
		;
		;foreach p:
		;	foreach k:
		;		foreach dt:
		;			calculate MAE for each action feature
		;	best = choose the one with the best validation error (lowest MAE among all features)
		(if (= k_folds 1)
			(call !AccumulateErrorsViaGridSearch)

			;else do k-fold validation, where each fold iterates over p k and dt and accumulates errors
			(call !AccumulateErrorsViaKFoldsGridSearch)
		)

		;convert the list of distances for each hyperparameter tuple into an avg distance
		;and while iterating store the best (smallest) distance and corresponding key
		(assign (assoc
			accumulated_results_map
				(map
					(lambda (let
						(assoc avg_distance (/ (apply "+" (get (current_value 1) "distances")) k_folds))

						(if (< avg_distance best_accuracy_distance)
							(assign (assoc
								best_accuracy_distance avg_distance
								best_params_key (current_index 1)
							))
						)

						;overwrite 'distances' with the average of the distances
						(set (current_value) "distances"
							avg_distance
						)
					))
					accumulated_results_map
				)
		))

		;sort the best params that all have the same 'best' accuracy distance by their P value
		(declare (assoc
			best_params
				(sort
					(lambda (< (get (current_value 1) "p") (get (current_value) "p")))
					(filter (lambda (= (get (current_value) "distances") best_accuracy_distance)) (values accumulated_results_map))
				)
			best_p (get accumulated_results_map (list best_params_key "p"))
		))

		;sort the best remaining params that have all the same 'best' p and accuracy distance by their K value
		(assign (assoc
			best_params
				(sort
					(lambda (< (get (current_value 1) "k") (get (current_value) "k")))
					;filter out all params that did not have a matching p
					(filter (lambda (= (get (current_value) "p") best_p)) best_params)
				)
		))

		;at this point best_params should either have one set, that we can use, or it's sorted by K values, take the median in that case
		(assign (assoc
			best
				(if (= (size best_params) 0)
					(first best_params)

					;take the median value
					(get best_params (floor (/ (size best_params) 2)))
				)

		))

		(assign (assoc
			output_map
				(assoc
					"k" (get best "k")
					"p" (get best "p")
					"dt" (get best "dt")
					;store the accuracy of this grid search
					"gridSearchError" (get best "distances")
				)
		))

		;output updated hyperparam map
		(append
			baseline_hyperparameter_map
			output_map
		)
	)

	;output dataset Mean Absolute Error (MAE) for the provided baseline_hyperparameter_map
	#!ComputeResidualForParameters
	(seq
		(declare (assoc
			sample_case_ids
				(if (!= (null) num_analysis_samples)
					(call !AllCases (assoc num num_analysis_samples rand_seed (rand)))
				)
			accumulated_kfold_errors (list)
		))

		(if (= k_folds 1)
			(call !CalculateMAE (assoc
				action_features action_features
				context_features context_features
				case_ids sample_case_ids
				ignore_exact_cases .true
				robust_residuals .false
				custom_hyperparam_map baseline_hyperparameter_map
				use_case_weights use_case_weights
				weight_feature weight_feature
			))

			;else do k-fold, average out the value across the k-folds
			(seq
				;The method ComputeAndAccumulateKFoldsMAE accumulates all k_fold maes into 'accumulated_kfold_errors'
				(call !AccumulateErrorsViaKFoldsGridSearch (assoc accumulate_error_method "!ComputeAndAccumulateKFoldsMAE"))
				(/ (apply "+" accumulated_kfold_errors) k_folds)
			)
		)
	)

	;accumulate dataset MAE during kfold validation using provided baseline_hyperparameter_map
	#!ComputeAndAccumulateKFoldsMAE
	(accum (assoc
		accumulated_kfold_errors
			(call !CalculateMAE (assoc
				action_features action_features
				context_features context_features
				case_ids validation_case_ids
				cases_already_removed .true
				robust_residuals .false
				custom_hyperparam_map baseline_hyperparameter_map
				use_case_weights use_case_weights
				weight_feature weight_feature
			))
	))

	;doing k-fold validation, so each validation block is 1/k_folds of all the cases
	#!AccumulateErrorsViaKFoldsGridSearch
	(declare
		(assoc
			validation_size
				(if (> k_folds 1)
					(/ num_cases k_folds)
					.infinity
				)
			;name of method to use for accumulating MAE, default to use accumulation during grid search
			accumulate_error_method "!AccumulateErrorsViaGridSearch"
		)

		;temporary entity storage for the validation cases
		(create_entities "_temp_" (null))

		;do k-fold cross validation by taking out k even chunks of all the cases and then validating on each one
		(map
			(lambda (let
				(assoc
					;as we iterate over values of 0 through (k_folds - 1), grab the corresponding 1st, 2nd, etc chunk of all cases by their indices
					;generate a list of indices that's validation_size in length, and starts at the correct offset based on
					;which k-folk "chunk" is being tested as specified by (current_value)
					validation_case_ids
						(call !AllCases (assoc
							num validation_size
							start_offset (* (current_value 2) validation_size)
						))
					p_k_dt_key (null)
				)

				;move this chunk of validation cases into _temp_ so that they aren't in the dataset during the reaction/validation process
				(map
					(lambda (move_entities (current_value) (list "_temp_" (current_value 1))) )
					validation_case_ids
				)

				;call !the method to compute the error
				(call (retrieve_from_entity accumulate_error_method))

				;restore the validation cases from backup
				(map
					(lambda (move_entities (list "_temp_" (current_value 1)) (current_value)) )
					validation_case_ids
				)
			))
			;k-fold, 0-based indexing
			(range 0 (- k_folds 1))
		)

		;no longer need the temporary entity container
		(destroy_entities "_temp_")
	)

	#!AccumulateErrorsViaGridSearch
	(map
		(lambda (let
			(assoc p_value (current_value 1))

			(map
				(lambda (let
					(assoc  k_value (current_value 1))
					(map
						(lambda (let
							(assoc
								dt_value (current_value 1)
							)

							;accumulate the accuracy distance for each set of parameters
							(declare (assoc
								accuracy_distance
									(call !CalculateMAE (append
										;params used in both cases
										(assoc
											action_features action_features
											context_features context_features
											robust_residuals .false
											use_case_weights use_case_weights
											weight_feature weight_feature
											custom_hyperparam_map
												;update the baseline hyperparams with new k, p, and dt
												(append
													baseline_hyperparameter_map
													(assoc "p" p_value "k" k_value "dt" dt_value)
												)
										)
										;extra conditional params
										(if (> k_folds 1)
											;iterate over all the validation_case_ids and react to each one
											;returning the mean absolute error (MAE) for each action feature
											(assoc
												p_param_categorical_mean p_param_categorical_mean
												p_param_accuracy_mean p_param_accuracy_mean
												case_ids validation_case_ids
												cases_already_removed .true
											)

											;else do hold-one-out validation
											(assoc
												case_ids sample_case_ids
												case_feature_list case_feature_list
												ignore_exact_cases .true
											)
										)
									))
							))

							(assign (assoc p_k_dt_key (concat p_value k_value dt_value)))

							;a map of   { k_p_dt_key : {  k: K, p: P, dt: DT, dist: [ ...] } }
							(assign (assoc
								accumulated_distances
									(if (= (null) (get accumulated_results_map (list p_k_dt_key "distances")))
										(list accuracy_distance)

										(append
											(get accumulated_results_map (list p_k_dt_key "distances"))
											accuracy_distance
										)
									)
							))

							;store both the distance calculated using the p value and the geometric mean
							(accum (assoc
								accumulated_results_map
									(associate
										p_k_dt_key
											(assoc
												"k" k_value
												"p" p_value
												"dt" dt_value
												"distances" accumulated_distances
											)
									)
							))
						))
						dt_values
					)
				))
				k_values
			)
		))
		p_values
	)

)