;Contains methods for the removal of cases.
(null

	;removes all cases that match the specified conditions from trainee
	#remove_cases
	(declare
		;returns {
		; 	type "assoc"
		; 	description "Map containing the count of how many cases were removed."
		;	additional_indices .false
		; 	indices {
		; 		"count" {type "number" required .true}
		; 	}
		; }
		(assoc
			;{ref "CaseIndices"}
			;a list of session id and training index tuples that specify which cases are to be removed
			case_indices (null)
			;{ref "Condition"}
			;assoc of feature->value(s) (no value or (null) = select cases where that feature is (null)/missing, one value = must equal exactly the value, two values = inclusive between with support for (null) on either side for less than/greater than conditions, [(null) (null)] = select all non-null cases). Ignored if case_indices is specified.
			condition (assoc)
			;{type "string"}
			;if specified, ignores condition and instead operates on all cases that were trained with this session id. Ignored if case_indices is specified.
			condition_session (null)
			;{ref "Precision"}
			;flag, whether to query for 'exact' matches; if set to 'similar' will remove num_cases with the most similar values. Ignored if case_indices is specified.
			precision "exact"
			;{type "number" min 0}
			;limit on the number of cases to move; If set to zero there will be no limit. Ignored if case_indices is specified.
			;	If null, will be set to k if precision is "similar" or no limit if precision is "exact". default is null
			num_cases (null)
			;{type "string"}
			;name of feature into which to distribute the removed cases' weights to their neighbors.
			distribute_weight_feature (null)
		)
		(call !ValidateParameters)
		(call !ValidateCondition)

		;build list of case ids that match criteria
		(declare (assoc
			cases_to_remove
				(if (= (null) case_indices)
					;compute the cases to re/move
					(call !GetCasesByCondition (assoc
						condition condition
						condition_session condition_session
						precision precision
						num_cases num_cases
					))

					;else use case_indices
					(call !GetCaseIds (assoc case_indices case_indices))
				)
		))

		;remove cases and do session cleanup
		(call !RemoveCases (assoc
			cases cases_to_remove
			distribute_weight_feature distribute_weight_feature
		))

		(accum_to_entities (assoc !revision 1))

		;return the number of cases moved
		(call !Return (assoc payload (assoc "count" (size cases_to_remove)) ))
	)

	;removes all contained entities that are cases except those that are complete contexts and actions
	; as those containing all of context_features or action_features
	;any additional cases to keep should be passed in as keys to the optional parameter cases_to_keep (values don't matter)
	#!RemoveIncompleteCases
	(declare
		(assoc
			context_features (list)
			action_features (list)

			;keys represent contexts that should be kept (for quick lookup), values are null
			cases_to_keep_map (assoc)
		)

		(let
			(assoc
				features
					(if (= 0 (size context_features) (size action_features))
						!trainedFeatures
						(append context_features action_features)
					)
			)

			;populate cases_to_keep_map
			(assign (assoc
				cases_to_keep_map
					(zip
						;cases
						(contained_entities
							(map
								(lambda (query_exists (current_value)) )
								features
							)
						)
					)
			))
		)

		(if (!= (size cases_to_keep_map) (call !GetNumTrainingCases))
			;dataset has changed so clear out these cached value
			(call !ClearCachedCountsAndEntropies)
		)

		;destroy any cases that aren't in cases_to_keep_map
		(map
			;destroy any entities
			(lambda (destroy_entities (current_value)))

			;that have empty lists of empty replay references
			(filter
				(lambda
					(not (contains_index cases_to_keep_map (current_value)))
				)

				(call !AllCases)
			)
		)
	)

	;goes through every session and removes any cases within the replay that are invalid
	#!RemoveInvalidCasesFromSessionReplay
	(map
		(lambda (assign_to_entities
			(current_value)
			(assoc
				".replay_steps"
					(filter
						(lambda (and
							;case should not be null
							(!= (null) (current_value))
							;case should exist
							(contains_entity (current_value))
						))
						(retrieve_from_entity (current_value 1) ".replay_steps")
					)
			)
		))

		(call !GetSessionIds)
	)

	;removes any unused cases from trainee
	#!RemoveCasesWithoutSessionReferences
	(let
		(assoc sessions_map (zip (call !GetSessionIds)))

		(map
			;destroy any entities
			(lambda (destroy_entities (current_value) ))

			;keep only those cases that don't have a session or non-existent sessions
			(filter
				(lambda
					(or
						(= (retrieve_from_entity (current_value) !internalLabelSession) (null))
						(not (contains_index sessions_map (retrieve_from_entity (current_value) !internalLabelSession)))
					)
				)

				(call !AllCases)
			)
		)

		;dataset has changed so clear out these cached value
		(call !ClearCachedCountsAndEntropies)

		.true
	)

	;Remove cases and cleanup session entities replay steps and update all the remaining cases' session indices
	;parameters:
	; cases: list of case ids to remove
	; distribute_weight_feature: name of feature into which to distribute the removed cases' weights to their neighbors.
	#!RemoveCases
	(declare
		(assoc
			cases (list)
			distribute_weight_feature (null)
		)

		(if (= 0 (size cases)) (conclude))

		;need to distribute the weight feature values from these removed cases to neighbors
		(if distribute_weight_feature
			(let
				(assoc has_rebalance_features (or !continuousRebalanceFeatures !nominalRebalanceFeatures) )
				;CreateCaseWeights will find all cases missing distribute_weight_feature, then initialize
				;distribute_weight_feature with a value of 1.0 for each
				(call !CreateCaseWeights (assoc
					feature_name distribute_weight_feature
					has_rebalance_features has_rebalance_features
				))
				(call !DistributeCaseInfluenceWeights (assoc
					features !trainedFeatures
					case_ids cases
					distribute_weight_feature distribute_weight_feature
					has_rebalance_features has_rebalance_features
				))
			)

			(seq
				;accumulate data mass change equivalent to one feature being changed for each case that satisfies the condition.
				(accum_to_entities (assoc
					!dataMassChangeSinceLastAnalyze (size cases)
					!dataMassChangeSinceLastDataReduction (size cases)
				))

				;must reduce cached rebalance and probability masses for proper rebalancing
				(if (or !continuousRebalanceFeatures !nominalRebalanceFeatures)
					(let
						(assoc
							removed_probability_mass
								(compute_on_contained_entities
									(query_in_entity_list cases)
									(query_exists !internalLabelProbabilityMass)
									(query_sum !internalLabelProbabilityMass)
								)
							removed_rebalance_weight
								(compute_on_contained_entities
									(query_in_entity_list cases)
									(query_exists ".case_weight")
									(query_sum ".case_weight")
								)
							;approximate the scalar for all the reductions
							scalar (/ !cachedTotalMass !cachedRebalanceTotalMass)
							num_remaining_cases (- (call !GetNumTrainingCases) (size cases))
						)
						(assign_to_entities (assoc
							!cachedTotalMass (- !cachedTotalMass removed_probability_mass)
							!cachedRebalanceTotalMass
								(max
									(- !cachedRebalanceTotalMass (/ removed_rebalance_weight scalar) )
									;prevent from setting the value from being too small unless there are no cases left, in which case 0 is correct
									(if num_remaining_cases (/ 1 (* num_remaining_cases scalar)) 0)
								)
						))
					)
				)
			)
		)

		;iterate over every session, cleaning up its replay steps and updating its cases' training indices
		(declare (assoc
			sessions_map
				(filter
					(lambda (let
						(assoc
							session (current_index 1)
							replay_steps (retrieve_from_entity (current_index 1) ".replay_steps")
						)

						(declare (assoc
							removed_cases_index_map
								;assoc of case id -> training index
								(map
									(lambda (first (current_value)))
									(compute_on_contained_entities
										(query_in_entity_list cases)
										(query_equals !internalLabelSession session)
										(query_exists !internalLabelSessionTrainingIndex)
									)
								)
						))

						;leave this session since it can be removed because it has no cases left
						(if (>= (size removed_cases_map) (size replay_steps))
							(conclude .true)
						)

						;filter cases from replay steps, leave only those cases that have not been removed
						(assign (assoc
							replay_steps (filter (lambda (not (contains_index removed_cases_index_map (current_value)))) replay_steps)
						))

						(assign_to_entities session (assoc
							".replay_steps" replay_steps
							".indices_map"
								;.indices_map is an assoc of train_index->case_id, so we can just simply delete all the removed training_indices with one delete call
								(remove
									(retrieve_from_entity session ".indices_map")
									(values removed_cases_index_map)
								)
						))

						;don't remove this session because it has cases left
						.false
					))
					;assoc/set of unique session ids.
					(zip (values
						;assoc of each case -> session id
						(map
							(lambda (first (current_value)))
							(compute_on_contained_entities (query_in_entity_list cases) (query_exists !internalLabelSession))
						)
					))
				)
		))

		;remove any empty sessions from the dataset
		(if (size sessions_map)
			(apply "destroy_entities" (indices sessions_map))
		)

		;remove all the cases after clearing the query caches
		(reclaim_resources (null) .false .true)
		(apply "destroy_entities" cases)


		;dataset has changed so clear out these cached value
		(call !ClearCachedCountsAndEntropies)

		(call !UpdateHasNulls (assoc features !trainedFeatures))

		(call !UpdateRegionalMinSize (assoc
			dataset_size (call !GetNumTrainingCases)
		))

		(null)
	)

	;Helper method to update !regionalMinSize to 30 for any dataset over 81 since 30 * e ~ 81.5
	#!UpdateRegionalMinSize
	(assign_to_entities (assoc
		!regionalMinSize
			(if (> dataset_size 81)
				30

				;else set it to dataset_size / e
				(/ dataset_size 2.71828182845)
			)
	))

)