;Contains methods for the removal of cases.
(null

	;removes all cases that match the specified conditions from trainee
	#remove_cases
	(declare
		;returns {
		; 	type "assoc"
		; 	description "Map containing the count of how many cases were removed."
		;	additional_indices .false
		; 	indices {
		; 		"count" {type "number" required .true}
		; 	}
		; }
		(assoc
			;{ref "CaseIndices"}
			;a list of session id and training index tuples that specify which cases are to be removed
			case_indices (null)
			;{ref "Condition"}
			;assoc of feature->value(s) (no value or (null) = select cases where that feature is (null)/missing, one value = must equal exactly the value, two values = inclusive between with support for (null) on either side for less than/greater than conditions, [(null) (null)] = select all non-null cases). Ignored if case_indices is specified.
			condition (assoc)
			;{type "string"}
			;if specified, ignores condition and instead operates on all cases that were trained with this session id. Ignored if case_indices is specified.
			condition_session (null)
			;{ref "Precision"}
			;flag, whether to query for 'exact' matches; if set to 'similar' will remove num_cases with the most similar values. Ignored if case_indices is specified.
			precision "exact"
			;{type "number" min 0}
			;limit on the number of cases to move; If set to zero there will be no limit. Ignored if case_indices is specified.
			;	If null, will be set to k if precision is "similar" or no limit if precision is "exact". default is null
			num_cases (null)
			;{type "string"}
			;name of feature into which to distribute the removed cases' weights to their neighbors.
			distribute_weight_feature (null)
		)
		(call !ValidateParameters)
		(call !ValidateCondition)

		;build list of case ids that match criteria
		(declare (assoc
			cases_to_remove
				(if (= (null) case_indices)
					;compute the cases to re/move
					(call !GetCasesByCondition (assoc
						condition condition
						condition_session condition_session
						precision precision
						num_cases num_cases
					))

					;else use case_indices
					(call !GetCaseIds (assoc case_indices case_indices))
				)
		))

		;remove cases and do session cleanup
		(call !RemoveCases (assoc
			cases cases_to_remove
			distribute_weight_feature distribute_weight_feature
		))

		(accum_to_entities (assoc !revision 1))

		;return the number of cases moved
		(call !Return (assoc payload (assoc "count" (size cases_to_remove)) ))
	)

	;removes all contained entities that are cases except those that are complete contexts and actions
	; as those containing all of context_features or action_features
	;any additional cases to keep should be passed in as keys to the optional parameter cases_to_keep (values don't matter)
	#!RemoveIncompleteCases
	(declare
		(assoc
			context_features (list)
			action_features (list)

			;keys represent contexts that should be kept (for quick lookup), values are null
			cases_to_keep_map (assoc)
		)

		(let
			(assoc
				features
					(if (= 0 (size context_features) (size action_features))
						!trainedFeatures
						(append context_features action_features)
					)
			)

			;populate cases_to_keep_map
			(assign (assoc
				cases_to_keep_map
					(zip
						;cases
						(contained_entities
							(map
								(lambda (query_exists (current_value)) )
								features
							)
						)
					)
			))
		)

		(if (!= (size cases_to_keep_map) (call !GetNumTrainingCases))
			;dataset has changed so clear out these cached value
			(call !ClearCachedCountsAndEntropies)
		)

		;destroy any cases that aren't in cases_to_keep_map
		(map
			;destroy any entities
			(lambda (destroy_entities (current_value)))

			;that have empty lists of empty replay references
			(filter
				(lambda
					(not (contains_index cases_to_keep_map (current_value)))
				)

				(call !AllCases)
			)
		)
	)

	;goes through every session and removes any cases within the replay that are invalid
	#!RemoveInvalidCasesFromSessionReplay
	(map
		(lambda (assign_to_entities
			(current_value)
			(assoc
				".replay_steps"
					(filter
						(lambda (and
							;case should not be null
							(!= (null) (current_value))
							;case should exist
							(contains_entity (current_value))
						))
						(retrieve_from_entity (current_value 1) ".replay_steps")
					)
			)
		))

		(call !GetSessionIds)
	)

	;removes any unused cases from trainee
	#!RemoveCasesWithoutSessionReferences
	(let
		(assoc sessions_map (zip (call !GetSessionIds)))

		(map
			;destroy any entities
			(lambda (destroy_entities (current_value) ))

			;keep only those cases that don't have a session or non-existent sessions
			(filter
				(lambda
					(or
						(= (retrieve_from_entity (current_value) !internalLabelSession) (null))
						(not (contains_index sessions_map (retrieve_from_entity (current_value) !internalLabelSession)))
					)
				)

				(call !AllCases)
			)
		)

		;dataset has changed so clear out these cached value
		(call !ClearCachedCountsAndEntropies)

		.true
	)

	;Remove cases and cleanup session entities replay steps and update all the remaining cases' session indices
	;parameters:
	; cases: list of case ids to remove
	; distribute_weight_feature: name of feature into which to distribute the removed cases' weights to their neighbors.
	#!RemoveCases
	(declare
		(assoc
			cases (list)
			distribute_weight_feature (null)
		)

		(if (= 0 (size cases)) (conclude))

		;list of session ids, one per case
		(declare (assoc
			sessions (map (lambda (retrieve_from_entity (current_value) !internalLabelSession)) cases)
			train_indices (map (lambda (retrieve_from_entity (current_value) !internalLabelSessionTrainingIndex)) cases)
		))

		(declare (assoc
			;map of session -> list of removed case ids. If a session only has one case id, it will be by itself and not in a list.
			sessions_map
				(zip
					(lambda (append (current_value 1) (current_value)))
					;indices are sessions, which when clobbered will append the case id to the growing list of ids for each session
					sessions
					cases
				)
			;map of session -> list of removed case training indices
			session_train_indices_map
				(zip
					(lambda (append (current_value 1) (current_value)))
					;indices are sessions, which when clobbered will append the train_index to the growing list of train_indices for each session
					sessions
					train_indices
				)
		))

		;need to distribute the weight feature values from these removed cases to neighbors
		(if distribute_weight_feature
			(let
				(assoc has_rebalance_features (or !continuousRebalanceFeatures !nominalRebalanceFeatures) )
				;CreateCaseWeights will find all cases missing distribute_weight_feature, then initialize
				;distribute_weight_feature with a value of 1.0 for each
				(call !CreateCaseWeights (assoc
					feature_name distribute_weight_feature
					has_rebalance_features has_rebalance_features
				))
				(call !DistributeCaseInfluenceWeights (assoc
					features !trainedFeatures
					case_ids cases
					distribute_weight_feature distribute_weight_feature
					has_rebalance_features has_rebalance_features
				))
			)

			(seq
				;accumulate data mass change equivalent to one feature being changed for each case that satisfies the condition.
				(accum_to_entities (assoc
					!dataMassChangeSinceLastAnalyze (size cases)
					!dataMassChangeSinceLastDataReduction (size cases)
				))

				;must reduce cached rebalance and probability masses for proper rebalancing
				(if (or !continuousRebalanceFeatures !nominalRebalanceFeatures)
					(let
						(assoc
							removed_probability_mass
								(compute_on_contained_entities
									(query_in_entity_list cases)
									(query_exists !internalLabelProbabilityMass)
									(query_sum !internalLabelProbabilityMass)
								)
							removed_rebalance_weight
								(compute_on_contained_entities
									(query_in_entity_list cases)
									(query_exists ".case_weight")
									(query_sum ".case_weight")
								)
							;approximate the scalar for all the reductions
							scalar (/ !cachedTotalMass !cachedRebalanceTotalMass)
							num_remaining_cases (- (call !GetNumTrainingCases) (size cases))
						)
						(assign_to_entities (assoc
							!cachedTotalMass (- !cachedTotalMass removed_probability_mass)
							!cachedRebalanceTotalMass
								(max
									(- !cachedRebalanceTotalMass (/ removed_rebalance_weight scalar) )
									;prevent from setting the value from being too small unless there are no cases left, in which case 0 is correct
									(if num_remaining_cases (/ 1 (* num_remaining_cases scalar)) 0)
								)
						))
					)
				)
			)
		)

		(declare (assoc re_derivation_series_case_ids (null) ))

		;check if time series dataset, if so, need to pull all cases from affected series so that their
		;derived features can be-rederived below after cases are removed
		(if (and (!= (null) !tsTimeFeature) (size !derivedFeaturesMap))
			(let
				(assoc series_id_features (get !tsFeaturesMap "series_id_features") )

				;for each case pull the list of series id values for all the series id features
				(declare (assoc
					series_id_lists
						(values (map
							(lambda (retrieve_from_entity (current_value) series_id_features))
							cases
						) .true)
				))

				;for each series affected, get the cases so they can be re-derived
				(declare (assoc
					per_series_cases
						(map
							(lambda (let
								(assoc
									series_id_values (current_value 1)
								)

								(contained_entities
									(values (map
										(lambda
											(query_equals (current_index) (current_value))
										)
										(zip series_id_features series_id_values)
									))
								)
							))
							series_id_lists
						)
				))

				;collapse all lists of cases into one list of unique case ids
				(assign (assoc
					re_derivation_series_case_ids (apply "append" per_series_cases)
				))
			)
		)

		;remove all the cases
		(apply "destroy_entities" cases)

		;iterate over every session, cleaning up its replay steps and uptating its cases' training indices
		(map
			(lambda (let
				(assoc
					session (current_index 1)
					replay_steps (retrieve_from_entity (current_index 1) ".replay_steps")
					removed_cases_map
						;if there's only one case for this session, its size will be 0, wrap it in a list prior to zipping
						(if (= 0 (size (current_value 1)))
							(zip (list (current_value 2)))
							(zip (current_value 1))
						)
				)

				;filter cases from replay steps, leave only those cases that have not been removed
				(assign (assoc
					replay_steps (filter (lambda (not (contains_index removed_cases_map (current_value)))) replay_steps)
				))

				(assign_to_entities session (assoc
					".replay_steps" replay_steps
					".indices_map"
						;.indices_map is an assoc of train_index->case_id, so we can just simply delete all the removed training_indices with one delete call
						(remove
							(retrieve_from_entity session ".indices_map")
							(get session_train_indices_map session)
						)
				))
			))
			sessions_map
		)

		;dataset has changed so clear out these cached value
		(call !ClearCachedCountsAndEntropies)

		;if there are cases that need to have features re-derived, do it here
		(if re_derivation_series_case_ids
			(call !DeriveTrainFeatures (assoc
				features !trainedFeatures
				;keep and derive only those features that are not in the features list
				derived_features (get !tsFeaturesMap "ts_derived_features")
				case_ids re_derivation_series_case_ids
			))
		)

		(call !UpdateHasNulls (assoc features !trainedFeatures))

		(call !UpdateRegionalMinSize (assoc
			dataset_size (call !GetNumTrainingCases)
		))

		(null)
	)

	;Helper method to update !regionalMinSize to 30 for any dataset over 81 since 30 * e ~ 81.5
	#!UpdateRegionalMinSize
	(assign_to_entities (assoc
		!regionalMinSize
			(if (> dataset_size 81)
				30

				;else set it to dataset_size / e
				(/ dataset_size 2.71828182845)
			)
	))

)